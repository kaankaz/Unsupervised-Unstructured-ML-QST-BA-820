{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ozalcZj6GnO"
   },
   "source": [
    "# BA 820 Homework 3 (100 Points)\n",
    "\n",
    "Group Member Names: KAAN KAZANCOGLU\n",
    "\n",
    "Reminder: you should not be sharing code across groups\n",
    "\n",
    "Please submit 1) PDF answers and 2) python notebook. Grading will be based on the homework answer write up PDF. Python notebook is for reference and back up only. So please make sure that your all the outputs and answers are clearly visible in the pdf.\n",
    "\n",
    "## 1 Latent Dirichlet Allocation [60pts]\n",
    "\n",
    "In this problem, we will use Latent Dirichlet Allocation to perform topic modeling on Amazon Review datasets. In particular, we will take an in-depth look at different aspects of LDA model.\n",
    "\n",
    "## 1.1 Installation\n",
    "\n",
    "To perform LDA and visualize, please use Python 3.X. You will also need to install Numpy, Scipy, gensim, nltk, pyLDAvis library. Refer to requirements.txt for more details.\n",
    "Use the following code to install the labraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "K7m0zKTQLP-a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.22.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyldavis in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (3.4.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (1.22.4)\n",
      "Requirement already satisfied: setuptools in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (66.1.1)\n",
      "Requirement already satisfied: gensim in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (4.1.2)\n",
      "Requirement already satisfied: funcy in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (1.18)\n",
      "Requirement already satisfied: jinja2 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (2.11.3)\n",
      "Requirement already satisfied: numexpr in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (2.8.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (1.2.0)\n",
      "Requirement already satisfied: pandas>=1.3.4 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (1.4.2)\n",
      "Requirement already satisfied: scipy in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (1.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.3.4->pyldavis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.3.4->pyldavis) (2021.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=1.0.0->pyldavis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from gensim->pyldavis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from jinja2->pyldavis) (2.0.1)\n",
      "Requirement already satisfied: packaging in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from numexpr->pyldavis) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.3.4->pyldavis) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from packaging->numexpr->pyldavis) (3.0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.22.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim\n",
    "# install gensim for LDA\n",
    "%pip install nltk \n",
    "# install nltk to preprocess sentences\n",
    "%pip install pyldavis\n",
    "# to visualize LDA topics\n",
    "%pip install matplotlib \n",
    "# for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFDfBgMF6Rsu"
   },
   "source": [
    "The cell below tests if the packages we need have been installed correctly, and that we are in the correct environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9iRwy_ck6SLL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kaankazancoglu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "import pyLDAvis\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import gzip # to unzip the data\n",
    "import re # to replace punctuations\n",
    "from nltk.corpus import stopwords # list of stopwords\n",
    "#The libraries I added:\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import TfidfModel, LsiModel\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLu3zAuv6m0r"
   },
   "source": [
    "## 1.2 Datasets\n",
    "\n",
    "You can download the Amazon reviews dataset of Cellphones & Accessory 5-Core Data [here](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz). Place the downloaded dataset in the same folder as this notebook. You can use the following code to read a datat from GZIp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gbSEVGKw6nV7"
   },
   "outputs": [],
   "source": [
    "# A function to read the zipped data at a specfic path\n",
    "#\n",
    "# How to use:\n",
    "# PATH = \"/path/to/file\"\n",
    "# for line in parse(PATH):\n",
    "#   do something with line\n",
    "#\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B48OODRf70mY"
   },
   "source": [
    "## 1.3 Data Cleaning\n",
    "\n",
    "Now we will preprocess the data using the following steps:\n",
    "   1. Remove stopwords\n",
    "   2. Lower-case all words\n",
    "   3. Remove words with less than 2 characters\n",
    "   4. Remove punctuation\n",
    "   5. Split each sentence into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6JVvT5kh7qO9"
   },
   "outputs": [],
   "source": [
    "# A function to clean a single line of text\n",
    "def clean_line(line):\n",
    "    \"\"\" Clean stopwords and punction for each line\n",
    "    \n",
    "    Args: \n",
    "        line (string): one line in file\n",
    "        \n",
    "    Returns:\n",
    "        list(str): a list of all words in the sentence\n",
    "    \"\"\"\n",
    "    punctuationRegex = r'\\W+|\\d+'\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    line = line.split(\" \")\n",
    "    filtered_content = []\n",
    "    for word in line:\n",
    "        #########################\n",
    "        if word.lower() not in stopWords:\n",
    "            word = word.lower()\n",
    "            word = re.sub(punctuationRegex, '', word)\n",
    "        if word.lower() not in stopWords and len(word) >= 2:\n",
    "            filtered_content.append(word)\n",
    "        #########################\n",
    "    return filtered_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats', 'dogs', 'wondering', 'imagined', 'imagined']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking to see if clean_title works properly\n",
    "clean_line(\"I can only cats dogs wondering imagined should imagined can won't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYnnn1KC8jZ8"
   },
   "source": [
    "Finally, we put parse() and clean_line() function together and then extract the first 10,000 reviews into a new text file as your experiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FXjJSwnD8jvK"
   },
   "outputs": [],
   "source": [
    "def read_dataset(fname):\n",
    "    \"\"\" Read the 10000 lines in given dataset into list and clean stop words. \n",
    "        \n",
    "    Args: \n",
    "        fname (string): filename of Amazon Review Dataset\n",
    "        \n",
    "    Returns:\n",
    "        list of list of words: we view each document as a list, including a list of all words \n",
    "    \"\"\"\n",
    "    count = 1\n",
    "    exp_dataset = []\n",
    "    for review in parse(fname):\n",
    "        line = review[\"reviewText\"]\n",
    "        new_line = clean_line(line)\n",
    "        exp_dataset.append(new_line)\n",
    "        count += 1\n",
    "        if count > 10000:\n",
    "            break\n",
    "    return exp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4cRY1JaN8mIM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.8 s, sys: 243 ms, total: 3.04 s\n",
      "Wall time: 3.04 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "r = read_dataset(\"reviews_Cell_Phones_and_Accessories_5.json.gz\")\n",
    "len(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FQFnMjb8l1B"
   },
   "source": [
    "## 1.4 Topic Analysis\n",
    "\n",
    "**[5pts] Q1.4.1.1** Use topic numbers 3, 6, 9, 12, 15 respectively and print out all topics with 5 words.\n",
    "\n",
    "For this We will use gensim to train an LDA model. gensim requires the following steps:\n",
    "\n",
    "Construct a gensim.corpora.dictionary from the dataset\n",
    "Construct a gensim \"corpus\" using this dictionary, by mapping each word to an index in the dictionary\n",
    "Run LDA on this corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "JzfOjF9I9kDD"
   },
   "outputs": [],
   "source": [
    "# Dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(r) # create a gensim dictionary, store it in variable \"dictionary\"\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(text) for text in r] # create the gensim corpus, store it in variable \"corpus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xblcs_6-99gp"
   },
   "source": [
    "The function below prints the top num words in each topic for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "b_NopMJj951v"
   },
   "outputs": [],
   "source": [
    "def print_topic_words(model,num):\n",
    "    \"\"\" print top words in model topics.\n",
    "    \n",
    "    Args: \n",
    "        model: LDA model\n",
    "        \n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"    \n",
    "    #########################\n",
    "    for i, topic in enumerate(model.print_topics(num_words=num)):\n",
    "        print ('Topic {} --- {}'.format(i, topic[1]))\n",
    "    #########################\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56Sv1l9S-JeI"
   },
   "source": [
    "The following function builds multiple LDA models with number of topics specified in the list `num_topics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NhGPQYTo-Jxx"
   },
   "outputs": [],
   "source": [
    "def build_num_topic_model(dictionary, corpus, num_topics):\n",
    "    \"\"\" Build lda model with given parameters, use print_topic_words to print words\n",
    "    \n",
    "    Args: \n",
    "        dictionary: dictionary built from dataset\n",
    "        corpus: corpus built from dataset\n",
    "        num_topics: list of numbers\n",
    "        \n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"    \n",
    "    for num_topic in num_topics:\n",
    "        #########################\n",
    "        print(num_topic,\"Topics:\")\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                    id2word=dictionary,\n",
    "                                                    num_topics=num_topic, \n",
    "                                                    eta = 'symmetric',\n",
    "                                                    alpha ='symmetric')\n",
    "        print_topic_words(lda_model,5)\n",
    "        #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "k8NtFThx-Od0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Topics:\n",
      "Topic 0 --- 0.011*\"use\" + 0.011*\"ear\" + 0.008*\"like\" + 0.006*\"iphone\" + 0.006*\"well\"\n",
      "Topic 1 --- 0.020*\"phone\" + 0.011*\"headset\" + 0.007*\"one\" + 0.007*\"bluetooth\" + 0.007*\"great\"\n",
      "Topic 2 --- 0.020*\"phone\" + 0.015*\"case\" + 0.014*\"one\" + 0.009*\"great\" + 0.009*\"iphone\"\n",
      "6 Topics:\n",
      "Topic 0 --- 0.026*\"phone\" + 0.012*\"screen\" + 0.008*\"use\" + 0.008*\"like\" + 0.006*\"get\"\n",
      "Topic 1 --- 0.017*\"headset\" + 0.015*\"ear\" + 0.012*\"sound\" + 0.011*\"bluetooth\" + 0.010*\"quality\"\n",
      "Topic 2 --- 0.026*\"phone\" + 0.022*\"case\" + 0.012*\"one\" + 0.010*\"get\" + 0.009*\"screen\"\n",
      "Topic 3 --- 0.017*\"one\" + 0.015*\"phone\" + 0.007*\"good\" + 0.007*\"cable\" + 0.007*\"product\"\n",
      "Topic 4 --- 0.025*\"case\" + 0.015*\"iphone\" + 0.013*\"phone\" + 0.009*\"one\" + 0.008*\"good\"\n",
      "Topic 5 --- 0.016*\"phone\" + 0.015*\"charge\" + 0.014*\"charger\" + 0.014*\"battery\" + 0.013*\"works\"\n",
      "9 Topics:\n",
      "Topic 0 --- 0.037*\"phone\" + 0.011*\"use\" + 0.007*\"great\" + 0.006*\"like\" + 0.006*\"get\"\n",
      "Topic 1 --- 0.012*\"one\" + 0.011*\"great\" + 0.011*\"good\" + 0.010*\"phone\" + 0.009*\"quality\"\n",
      "Topic 2 --- 0.017*\"phone\" + 0.012*\"use\" + 0.009*\"get\" + 0.008*\"one\" + 0.007*\"iphone\"\n",
      "Topic 3 --- 0.033*\"case\" + 0.030*\"iphone\" + 0.022*\"cable\" + 0.010*\"phone\" + 0.009*\"works\"\n",
      "Topic 4 --- 0.012*\"phone\" + 0.010*\"well\" + 0.009*\"use\" + 0.009*\"screen\" + 0.008*\"one\"\n",
      "Topic 5 --- 0.034*\"charger\" + 0.032*\"charge\" + 0.016*\"usb\" + 0.012*\"car\" + 0.010*\"device\"\n",
      "Topic 6 --- 0.025*\"headset\" + 0.013*\"ear\" + 0.012*\"bluetooth\" + 0.012*\"sound\" + 0.011*\"one\"\n",
      "Topic 7 --- 0.031*\"phone\" + 0.025*\"case\" + 0.019*\"one\" + 0.011*\"like\" + 0.011*\"good\"\n",
      "Topic 8 --- 0.017*\"charger\" + 0.017*\"works\" + 0.017*\"great\" + 0.016*\"work\" + 0.013*\"one\"\n",
      "12 Topics:\n",
      "Topic 0 --- 0.015*\"one\" + 0.014*\"battery\" + 0.014*\"phone\" + 0.010*\"like\" + 0.010*\"charge\"\n",
      "Topic 1 --- 0.018*\"phone\" + 0.009*\"great\" + 0.008*\"product\" + 0.008*\"works\" + 0.007*\"good\"\n",
      "Topic 2 --- 0.010*\"phone\" + 0.008*\"film\" + 0.007*\"docking\" + 0.005*\"torch\" + 0.005*\"good\"\n",
      "Topic 3 --- 0.014*\"headset\" + 0.014*\"phone\" + 0.010*\"use\" + 0.009*\"one\" + 0.008*\"get\"\n",
      "Topic 4 --- 0.055*\"case\" + 0.023*\"phone\" + 0.019*\"iphone\" + 0.017*\"great\" + 0.016*\"screen\"\n",
      "Topic 5 --- 0.024*\"ear\" + 0.021*\"sound\" + 0.015*\"bluetooth\" + 0.014*\"headset\" + 0.011*\"quality\"\n",
      "Topic 6 --- 0.014*\"good\" + 0.013*\"works\" + 0.010*\"great\" + 0.009*\"like\" + 0.008*\"well\"\n",
      "Topic 7 --- 0.018*\"usb\" + 0.016*\"one\" + 0.015*\"charger\" + 0.014*\"cable\" + 0.013*\"charge\"\n",
      "Topic 8 --- 0.042*\"phone\" + 0.010*\"battery\" + 0.009*\"screen\" + 0.009*\"one\" + 0.008*\"like\"\n",
      "Topic 9 --- 0.031*\"phone\" + 0.009*\"like\" + 0.008*\"use\" + 0.008*\"good\" + 0.007*\"camera\"\n",
      "Topic 10 --- 0.013*\"screen\" + 0.013*\"phone\" + 0.012*\"case\" + 0.009*\"one\" + 0.008*\"belt\"\n",
      "Topic 11 --- 0.035*\"cable\" + 0.027*\"one\" + 0.013*\"cables\" + 0.013*\"iphone\" + 0.012*\"get\"\n",
      "15 Topics:\n",
      "Topic 0 --- 0.024*\"case\" + 0.019*\"one\" + 0.011*\"phone\" + 0.010*\"like\" + 0.009*\"get\"\n",
      "Topic 1 --- 0.013*\"de\" + 0.009*\"daughter\" + 0.009*\"charge\" + 0.007*\"battery\" + 0.007*\"voltage\"\n",
      "Topic 2 --- 0.015*\"blueant\" + 0.014*\"phone\" + 0.007*\"would\" + 0.007*\"docking\" + 0.005*\"great\"\n",
      "Topic 3 --- 0.024*\"works\" + 0.023*\"great\" + 0.017*\"price\" + 0.015*\"storm\" + 0.015*\"phone\"\n",
      "Topic 4 --- 0.015*\"iphone\" + 0.011*\"use\" + 0.009*\"phone\" + 0.008*\"headset\" + 0.008*\"would\"\n",
      "Topic 5 --- 0.025*\"cable\" + 0.013*\"apple\" + 0.011*\"phone\" + 0.008*\"use\" + 0.008*\"connector\"\n",
      "Topic 6 --- 0.021*\"headset\" + 0.016*\"sound\" + 0.015*\"bluetooth\" + 0.013*\"ear\" + 0.012*\"quality\"\n",
      "Topic 7 --- 0.056*\"phone\" + 0.023*\"case\" + 0.010*\"data\" + 0.009*\"would\" + 0.006*\"like\"\n",
      "Topic 8 --- 0.041*\"phone\" + 0.010*\"use\" + 0.009*\"signal\" + 0.008*\"cell\" + 0.008*\"one\"\n",
      "Topic 9 --- 0.027*\"battery\" + 0.015*\"works\" + 0.011*\"phone\" + 0.010*\"blackberry\" + 0.010*\"charger\"\n",
      "Topic 10 --- 0.023*\"one\" + 0.016*\"great\" + 0.012*\"phone\" + 0.011*\"use\" + 0.009*\"would\"\n",
      "Topic 11 --- 0.021*\"phone\" + 0.012*\"case\" + 0.011*\"cable\" + 0.010*\"iphone\" + 0.010*\"like\"\n",
      "Topic 12 --- 0.022*\"ear\" + 0.016*\"voyager\" + 0.014*\"good\" + 0.014*\"headset\" + 0.013*\"pro\"\n",
      "Topic 13 --- 0.026*\"iphone\" + 0.013*\"case\" + 0.010*\"use\" + 0.010*\"would\" + 0.007*\"stylus\"\n",
      "Topic 14 --- 0.031*\"screen\" + 0.020*\"phone\" + 0.011*\"protector\" + 0.009*\"case\" + 0.008*\"good\"\n"
     ]
    }
   ],
   "source": [
    "build_num_topic_model(dictionary, corpus, [3, 6, 9, 12, 15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8vkk6bo-UEi"
   },
   "source": [
    "**[3pts] Q1.4.1.2**  Explain what could be interpreted for each topics, and describe the similarity and difference between different topic numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRLNs647-VF7"
   },
   "source": [
    "-Based on the topics generated from the LDA model, it appears that the reviews are mainly about electronic devices, specifically phones, phone cases, headsets, chargers, and batteries. Other common keywords found in the topics include screen, sound quality, and bluetooth. It seems that reviews are mainly covering the features and performance of these devices and providing their opinions on their effectiveness, usability, and value for money.\n",
    "\n",
    "As the number of topics increases, the model becomes more specific and detailed in its grouping of words. For example, in the 12 topic model, there are more specific topics such as \"phone battery\", \"charging\", \"phone accessories like film and docking\", \"bluetooth headset and sound quality\" and \"USB charger and cable\". These topics are not present or less prominent in the 3 and 6 topic models. Also, for instance, in the 12-topic model, there are topics related to batteries, phone cases, USB chargers, and even torches. This indicates that the more topics that are used in the model, the more specific and varied the topics can become.\n",
    "\n",
    "Starting with similarities, we can see that the topic of \"phone\" is consistently present across all four models. This suggests that discussions about phones are common and prevalent across different contexts. Additionally, we can see that there are some common themes around phone accessories, such as cases, chargers, and headsets, which appear in multiple models.\n",
    "\n",
    "On the other hand, there are also some notable differences between the models. For example, in the 3-topic model, there is a clear distinction between topics related to phone use and topics related to phone accessories, while in the 9- and 12-topic models, there is more granularity in the discussion of specific types of accessories (e.g. chargers, cases, and headsets). Additionally, the 12-topic model includes some more specific topics, such as \"film\" and \"docking\", which do not appear in the other models.\n",
    "\n",
    "\n",
    "Overall, the main similarity between the different topic numbers is that they all identify groups of words that are related to each other in some way. However, as more topics are added, the model becomes more specific and detailed in its grouping of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERTTf2sGDIPp"
   },
   "source": [
    "**[2pts] Q1.4.1.3**  Which topic number would you choose? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOgDUPcvDLs8"
   },
   "source": [
    "-Higher number of topics can provide more granularity and diversity for the topics above. Based on my output, I would choose the number of topics as 12 as it offers a wider range of topics and potentially more nuanced insights compared to the other models. While using 12, I can get more specific topics which I did not get in lower topic numbers. In 15 topics, I have observed some overlaps between different topics that is why I'm choosing number 12. However, I need to check other features like perplexity and coherence to decide which number of topic best suits my data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJfqY1reDX5s"
   },
   "source": [
    "## 1.5 Model Evaluation\n",
    "\n",
    "**[12 pts] Q1.5.1** Now we investigate two methods to evaluate our model and choose the topic number\n",
    "\n",
    "1.Perplexity is a measurement of how well a probability distribution or probability model predicts a sample. A low perplexity indicates the probability distribution is good at predicting the sample. We can use model.log_perplexity(document) to evaluate the perplexity of our LDA model.\n",
    "\n",
    "2.Topic coherence is a one type of interpretability measurement for a topic. It measures if a set of top keywords describe a coherent and singular concept. A good topic will have high topic coherence score. We can use CoherenceModel(model=ldamodel).get_coherence() to calculate it.\n",
    "\n",
    "Plot Perplexity and topic coherence scores of our LDA model for topic number 3,6,9,12,15,20,50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni7eZRSIDsIU"
   },
   "source": [
    "The code below trains topic models with different numbers of topics and measures their coherence and perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "id": "1ue1Pf4--UjO"
   },
   "outputs": [],
   "source": [
    "# perplexity \n",
    "# run different number of topics to get perplexity and coherence value for this model\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "def get_measurement_for_model(dictionary, corpus, topic_nums):\n",
    "    \"\"\" Build lda model with given parameters \n",
    "    \n",
    "    Args: \n",
    "        dictionary: dictionary built from dataset\n",
    "        corpus: corpus built from dataset\n",
    "        topic_nums: a list contains all possible topic number\n",
    "        \n",
    "    Returns:\n",
    "        2 lists: one of perplexities, and one of coherence value\n",
    "    \"\"\"  \n",
    "    perplexity = []\n",
    "    coherence_value=[]\n",
    "    for num_topic in topic_nums:\n",
    "        #########################\n",
    "        #   - Build model\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=num_topic, \n",
    "                                           eta = 'symmetric',\n",
    "                                           alpha ='symmetric')\n",
    "        #   - Compute and store coherence\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, corpus=corpus,\n",
    "                                             dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        coherence_value.append(coherence_lda)\n",
    "        #   - Compute and store perplexity\n",
    "        perplexity_lda = lda_model.log_perplexity(corpus,total_docs= len(corpus))\n",
    "        perplexity.append(perplexity_lda)\n",
    "        #########################\n",
    "    return perplexity,coherence_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "id": "nX_zj0RoD4j0"
   },
   "outputs": [],
   "source": [
    "perplexity, coherence = get_measurement_for_model(dictionary, corpus, [3, 6, 9, 12, 15, 20, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "id": "yk-ojlJSD85k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.905933525965366, -7.996643039684271, -8.30467193989768, -8.57175976402896, -8.822069356133456, -9.121941515452475, -11.05889405451974]\n",
      "[-1.4920459691925236, -2.0262792595583954, -1.7407444682116688, -2.380980455367431, -2.335802396682216, -3.0430227957313205, -4.396026525155082]\n"
     ]
    }
   ],
   "source": [
    "print(perplexity)\n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcYTqBizD9az"
   },
   "source": [
    "We can now plot the coherence and perplexity of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "id": "1B9scv8sD_n8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "id": "G76jh0BXECKs"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([3, 6, 9, 12, 15, 20, 50], perplexity)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUDG-rr5ELQN"
   },
   "source": [
    "**[2pts] Q1.5.2**  From the above graph what topic number would you choose and why? Is it a good idea to choose the topic number based on perplexity? why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5SLdipiENoi"
   },
   "source": [
    "-Perplexity is a measure of how well the model predicts the test data. In general, a lower perplexity value indicates a better model fit. Based on solely perplexity values, I would choose 50 as it has the lowest perplexity value. However, perplexity is just one metric, it is not always parallel with coherence and interpretability. To evaluate the quality of an LDA model, I need to combine other metrics before deciding which number of topic is best for my LDA model. In a nutshell, while perplexity is a useful measure of model fit, it should not be the only feature for choosing the best topic number. Coherence and interpretability of topics are equally important factors to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "id": "EBhTyUQdELuW"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([3, 6, 9, 12, 15, 20, 50], coherence)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"Coherence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnCOd2_CEY2W"
   },
   "source": [
    "**[2pts] Q1.5.3**  From the above graph what topic number would you choose and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm5bcHV4EcOa"
   },
   "source": [
    "-We can see that the coherence score tends to decrease as the number of topics increases. This could indicate that the model is not able to effectively capture the structure of the data with more topics. Based on solely coherence scores, I'd choose topic number 9 since it has relatively high coherence compared to other models. The reason I do not go with number 3 is even though it has the highest coherence is, as discussed previously, low topic numbers are not really specific but more general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppxl4L4cElVA"
   },
   "source": [
    "**[4pts]Q1.5.4** Compare two methods you implemented in the previous quesions, which one do you think is better and why? In answering, please discuss the actual topics generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMiQEut8Em__"
   },
   "source": [
    "-When selecting the optimal number of topics one should choose a model that has low perplexity and high coherence. I think neither of them individually is perfect. We need to combine both methods while choosing number of topics. Topic 9 was able to capture topics seperately that includes keywords like bluetooth, charger, screen, cable, headset, iphone, usb, and car. When we use low topic numbers like 3, screen, car, usb are not included in top words. For high topic numbers like 15, we are seeing so many overlaps in topics within each other. Also, for instance, topic 10 in number of topic=15 > \"one\" + 0.016*\"great\" + 0.012*\"phone\" + 0.011*\"use\" + 0.009*\"would\" doesnt articulate what the actual topic is thus some topics become meaningless. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBkDhQzGFPyp"
   },
   "source": [
    "## 1.6 Alpha and Beta in LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jelhz4KFFgde"
   },
   "source": [
    "**[7pts]Q1.6.1** In this problem, we will check the two most important parameters in LDA model: alpha and beta. Alpha represents document-topic density - with a higher alpha, documents are made up of more topics, and with lower alpha, documents contain fewer topics. Beta represents topic-word density - with a high beta, topics are made up of most of the words in the corpus, and with a low beta they consist of few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "id": "x7Wr3DphEb2O"
   },
   "outputs": [],
   "source": [
    "best_topic_num = 9 # CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "id": "8AP4Hn1uFkkc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 --- 0.043*\"great\" + 0.022*\"works\" + 0.021*\"price\" + 0.021*\"product\" + 0.012*\"good\"\n",
      "Topic 1 --- 0.028*\"phone\" + 0.013*\"iphone\" + 0.013*\"case\" + 0.011*\"cable\" + 0.011*\"use\"\n",
      "Topic 2 --- 0.076*\"case\" + 0.016*\"protection\" + 0.014*\"good\" + 0.013*\"phone\" + 0.012*\"otterbox\"\n",
      "Topic 3 --- 0.013*\"charge\" + 0.012*\"phone\" + 0.009*\"great\" + 0.009*\"iphone\" + 0.009*\"droid\"\n",
      "Topic 4 --- 0.029*\"protector\" + 0.026*\"screen\" + 0.013*\"film\" + 0.012*\"bubbles\" + 0.011*\"protectors\"\n",
      "Topic 5 --- 0.014*\"headset\" + 0.012*\"bluetooth\" + 0.010*\"one\" + 0.010*\"use\" + 0.009*\"iphone\"\n",
      "Topic 6 --- 0.026*\"charger\" + 0.023*\"phone\" + 0.011*\"works\" + 0.011*\"samsung\" + 0.010*\"oem\"\n",
      "Topic 7 --- 0.027*\"phone\" + 0.011*\"one\" + 0.009*\"get\" + 0.008*\"screen\" + 0.008*\"like\"\n",
      "Topic 8 --- 0.013*\"headset\" + 0.012*\"one\" + 0.012*\"ear\" + 0.009*\"use\" + 0.008*\"sound\"\n",
      "Coherence for Model 1 is  0.3144981892009662\n",
      "Perplexity for Model 1 is  -8.300779483916715\n"
     ]
    }
   ],
   "source": [
    "#model 1\n",
    "model1 = None\n",
    "#########################\n",
    "# YOUR CODE HERE\n",
    "#   - Build model for alpha = 1/num_topic = eta\n",
    "model1 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                         id2word=dictionary,\n",
    "                                         num_topics=best_topic_num, \n",
    "                                         eta = 1/best_topic_num,\n",
    "                                         alpha =1/best_topic_num)\n",
    "\n",
    "#   - Print top words\n",
    "print_topic_words(model1,5)\n",
    "\n",
    "# Print Coherence\n",
    "print(\"Coherence for Model 1 is \", CoherenceModel(model=model1, texts=r,\n",
    "                                             dictionary=dictionary, coherence='c_v').get_coherence())\n",
    "# Print Perplexity\n",
    "print(\"Perplexity for Model 1 is \", model1.log_perplexity(corpus))\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "id": "3WDzHIzGFm7v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 --- 0.010*\"phone\" + 0.008*\"nokia\" + 0.007*\"data\" + 0.006*\"use\" + 0.005*\"great\"\n",
      "Topic 1 --- 0.018*\"phone\" + 0.011*\"one\" + 0.009*\"use\" + 0.009*\"charger\" + 0.007*\"would\"\n",
      "Topic 2 --- 0.010*\"headphones\" + 0.009*\"case\" + 0.009*\"bluetooth\" + 0.008*\"phone\" + 0.008*\"music\"\n",
      "Topic 3 --- 0.017*\"phone\" + 0.013*\"one\" + 0.013*\"great\" + 0.011*\"good\" + 0.011*\"screen\"\n",
      "Topic 4 --- 0.016*\"one\" + 0.015*\"phone\" + 0.011*\"great\" + 0.010*\"product\" + 0.008*\"like\"\n",
      "Topic 5 --- 0.020*\"headset\" + 0.014*\"ear\" + 0.010*\"sound\" + 0.010*\"one\" + 0.009*\"bluetooth\"\n",
      "Topic 6 --- 0.029*\"case\" + 0.020*\"phone\" + 0.009*\"would\" + 0.009*\"screen\" + 0.007*\"iphone\"\n",
      "Topic 7 --- 0.021*\"works\" + 0.020*\"cable\" + 0.017*\"work\" + 0.010*\"usb\" + 0.010*\"cables\"\n",
      "Topic 8 --- 0.031*\"phone\" + 0.015*\"battery\" + 0.009*\"get\" + 0.008*\"use\" + 0.007*\"like\"\n",
      "Coherence for Model 2 is  0.31512299280274886\n",
      "Perplexity for Model 2 is  -8.096258463276495\n"
     ]
    }
   ],
   "source": [
    "#model 2\n",
    "model2 = None\n",
    "#########################\n",
    "# YOUR CODE HERE\n",
    "#   - Build model for alpha = 1/2, eta = 1/5\n",
    "model2 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                         id2word=dictionary,\n",
    "                                         num_topics=best_topic_num, \n",
    "                                         eta = 1/5,\n",
    "                                         alpha =1/2)\n",
    "\n",
    "#   - Print top words\n",
    "print_topic_words(model2,5)\n",
    "\n",
    "# Print Coherence\n",
    "print(\"Coherence for Model 2 is \",CoherenceModel(model=model2, texts=r,\n",
    "                                             dictionary=dictionary, coherence='c_v').get_coherence())\n",
    "# Print Perplexity\n",
    "print(\"Perplexity for Model 2 is \", model2.log_perplexity(corpus))\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "id": "hhM9efkXFqI0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 --- 0.009*\"phone\" + 0.004*\"bare\" + 0.003*\"like\" + 0.003*\"prompt\" + 0.002*\"battery\"\n",
      "Topic 1 --- 0.017*\"iphone\" + 0.015*\"phone\" + 0.012*\"one\" + 0.009*\"battery\" + 0.009*\"great\"\n",
      "Topic 2 --- 0.025*\"battery\" + 0.013*\"phone\" + 0.010*\"one\" + 0.010*\"charge\" + 0.009*\"usb\"\n",
      "Topic 3 --- 0.008*\"bluetooth\" + 0.008*\"phone\" + 0.007*\"headset\" + 0.007*\"good\" + 0.006*\"get\"\n",
      "Topic 4 --- 0.016*\"ear\" + 0.013*\"headset\" + 0.010*\"great\" + 0.010*\"good\" + 0.008*\"bluetooth\"\n",
      "Topic 5 --- 0.021*\"one\" + 0.016*\"phone\" + 0.010*\"signal\" + 0.009*\"work\" + 0.009*\"use\"\n",
      "Topic 6 --- 0.021*\"cable\" + 0.017*\"charger\" + 0.015*\"works\" + 0.013*\"usb\" + 0.013*\"phone\"\n",
      "Topic 7 --- 0.021*\"phone\" + 0.014*\"headset\" + 0.010*\"use\" + 0.009*\"bluetooth\" + 0.009*\"like\"\n",
      "Topic 8 --- 0.045*\"case\" + 0.026*\"phone\" + 0.017*\"screen\" + 0.010*\"like\" + 0.010*\"good\"\n",
      "Coherence for Model 3 is  0.3326400494000536\n",
      "Perplexity for Model 3 is  -8.203319033343872\n"
     ]
    }
   ],
   "source": [
    "#model 3\n",
    "model3 = None\n",
    "#########################\n",
    "# YOUR CODE HERE\n",
    "#   - Build model for alpha = 'auto' = eta\n",
    "model3 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                         id2word=dictionary,\n",
    "                                         num_topics=best_topic_num, \n",
    "                                         eta = 'auto',\n",
    "                                         alpha ='auto')\n",
    "\n",
    "#   - Print top words\n",
    "print_topic_words(model3,5)\n",
    "\n",
    "# Print Coherence\n",
    "print(\"Coherence for Model 3 is \",CoherenceModel(model=model3, texts=r,\n",
    "                                             dictionary=dictionary, coherence='c_v').get_coherence())\n",
    "# Print Perplexity\n",
    "print(\"Perplexity for Model 3 is \", model3.log_perplexity(corpus))\n",
    "#########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRIg6fAIFwW0"
   },
   "source": [
    "**[3pts]1.6.2**  Explain how the different alpha and beta values theoretically influence the LDA model. Then describe what you find in the empirical result (e.g difference in topic words and topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU26drlOFxo1"
   },
   "source": [
    "-Alpha is a hyperparameter that controls the sparsity of the distribution over topics in documents. Beta is also a hyperparameter but it controls the sparsity of the distribution over words in topics. A higher value of alpha implies that the model expects each document to contain more topics, while a lower value of alpha implies that the model expects each document to contain fewer topics. On the contrary, for beta, higher value means that the model expects each topic to contain more words, while a lower value means that the model expects each topic to contain fewer words. In the empirical results, I observed that different alpha and beta values generates different topic keywords.\n",
    "\n",
    "model1: Alpha and eta are very low. When alpha is low, most of the weight in the topic distribution for the document goes to a single topic. When beta is low, higher weight placed on the top words and lower weight placed on the bottom words for each topic (or more intuitively, topics are composed of few words). For instance, Topic 2 --- 0.076*\"case\" + 0.016*\"protection\" + 0.014*\"good\" + 0.013*\"phone\" + 0.012*\"otterbox\", as we can see from topic 2, 'case' has the most weight by a wide margin compared to other words in topic 2. Also, topic 0, topic 2, topic 4 are much more dominant compared to other topics. Aferomentioned topics are related with phone case protectors and phone screen film protectors.\n",
    "\n",
    "\n",
    "model2: Alpha nad eta are bigger compared to model 1. When alpha is high, The weight is much more evenly distributed across the topics. When eta is high, model places relatively less weight on the top words and more weight on the bottom words. Thus high eta results in topics that have a smoother distribution of weight across all the words in the vocabulary. For instance, Topic 7 --- 0.021*\"works\" + 0.020*\"cable\" + 0.017*\"work\" + 0.010*\"usb\" + 0.010*\"cables\", as we can see from topic 7, all words are almost equally weighted. Also, topics are equally important across the document, which means Model 2 covers a broader range of topics including phone features (e.g., battery, screen) and accessories (e.g., case, headphones).\n",
    "\n",
    "\n",
    "model3: In auto, model automatically estimated the values of these hyperparameters using empirical Bayes method based on the data. Overall, the topics are more like the combination of first and second model, so it was more balanced compared to model1 and model2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSsPpXqXG11h"
   },
   "source": [
    "## 1.7 LDA on a short text dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbrCxteDHM1b"
   },
   "source": [
    "**[10pts]1.7.1** In this part, we will read a dataset from twitter and build a LDA model. On Windows, download and unzip the dataset from [this link](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip). Place the downloaded dataset in the same folder as this notebook. Use the first 10,000 lines in the \"training.1600000.processed.noemoticon.csv\" file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Eg8B4xaRFxA3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL transformed to HTTPS due to an HSTS policy\n",
      "--2023-03-04 18:36:24--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
      "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
      "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 81363704 (78M) [application/zip]\n",
      "Saving to: 'trainingandtestdata.zip.4'\n",
      "\n",
      "trainingandtestdata 100%[===================>]  77.59M  27.3MB/s    in 2.8s    \n",
      "\n",
      "2023-03-04 18:36:27 (27.3 MB/s) - 'trainingandtestdata.zip.4' saved [81363704/81363704]\n",
      "\n",
      "Archive:  trainingandtestdata.zip\n",
      "replace testdata.manual.2009.06.14.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip # Linux and OSX only\n",
    "!unzip trainingandtestdata.zip # Linux and OSX only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "81FoZvt_HTq6"
   },
   "outputs": [],
   "source": [
    "!head -n 10000 training.1600000.processed.noemoticon.csv > twitter.csv # Linux and OSX only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"twitter.csv\",header=None,encoding=\"utf-8\")\n",
    "df[5][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uJSwYJflHWvp"
   },
   "outputs": [],
   "source": [
    "def read_twitter(fname):\n",
    "    \"\"\" Read the given dataset into list and clean stop words. \n",
    "    \n",
    "    Args: \n",
    "        fname (string): filename of Twitter Dataset\n",
    "        \n",
    "    Returns:\n",
    "        list of list of words: we view each document as a list, including a list of all words \n",
    "    \"\"\"\n",
    "    ########################\n",
    "    stopword_en = nltk.corpus.stopwords.words('english')\n",
    "    stopword_es = nltk.corpus.stopwords.words('spanish')\n",
    "    stopword = stopword_en + stopword_es\n",
    "    punctuationRegex = r'\\W+|\\d+'\n",
    "    twt = []\n",
    "    ########################\n",
    "    twitter = []\n",
    "    with open(fname,encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            ########################\n",
    "            tweet = line.split(\",\")[5:]\n",
    "            tweet = \"\".join(tweet)\n",
    "            tweet = tweet.split(\" \")\n",
    "            for word in tweet:\n",
    "                word = nlp(word)\n",
    "                lemmas = [word.lemma_ for word in word]\n",
    "                for word in lemmas:\n",
    "                    if word.lower().startswith('@') or word.lower().startswith('#') or word.lower().startswith('http'):\n",
    "                        continue\n",
    "                    if word.lower() not in stopword:\n",
    "                        word = re.sub(punctuationRegex, '', word)\n",
    "                    if word.lower() not in stopword and len(word) >= 2:\n",
    "                        word = word.lower()\n",
    "                        twt.append(word)\n",
    "            twitter.append(twt)\n",
    "            twt = []       \n",
    "            ########################\n",
    "    return twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 49s, sys: 988 ms, total: 3min 50s\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])\n",
    "twitter = read_twitter('twitter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YzcgFZx_HZgK"
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "twitter_dictionary = Dictionary(twitter) # TODO: build dictionary\n",
    "twitter_corpus = [twitter_dictionary.doc2bow(doc) for doc in twitter] # TODO: build corpus for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3WoNVLuJHcyu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 --- 0.035*\"get\" + 0.018*\"go\" + 0.012*\"week\" + 0.012*\"time\" + 0.011*\"work\"\n",
      "Topic 1 --- 0.026*\"day\" + 0.016*\"go\" + 0.016*\"work\" + 0.013*\"today\" + 0.011*\"week\"\n",
      "Topic 2 --- 0.033*\"go\" + 0.013*\"want\" + 0.013*\"get\" + 0.011*\"back\" + 0.010*\"bed\"\n",
      "Topic 3 --- 0.019*\"really\" + 0.018*\"get\" + 0.017*\"want\" + 0.015*\"like\" + 0.014*\"know\"\n",
      "Topic 4 --- 0.038*\"work\" + 0.021*\"go\" + 0.015*\"get\" + 0.014*\"today\" + 0.013*\"love\"\n",
      "Topic 5 --- 0.034*\"get\" + 0.022*\"go\" + 0.020*\"feel\" + 0.017*\"sick\" + 0.017*\"snow\"\n",
      "Topic 6 --- 0.013*\"good\" + 0.012*\"one\" + 0.010*\"go\" + 0.010*\"new\" + 0.009*\"like\"\n",
      "Topic 7 --- 0.025*\"work\" + 0.022*\"go\" + 0.014*\"day\" + 0.014*\"one\" + 0.013*\"sad\"\n",
      "Topic 8 --- 0.020*\"get\" + 0.019*\"wish\" + 0.018*\"sorry\" + 0.018*\"miss\" + 0.016*\"go\"\n",
      "CPU times: user 2.62 s, sys: 23.6 ms, total: 2.64 s\n",
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "########################\n",
    "# YOUR CODE HERE\n",
    "twit_model = gensim.models.ldamodel.LdaModel(corpus = twitter_corpus,\n",
    "                                             id2word = twitter_dictionary, \n",
    "                                             num_topics = 9, \n",
    "                                             alpha = 'auto',\n",
    "                                             eta = 'auto')\n",
    "\n",
    "print_topic_words(twit_model, 5)\n",
    "\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzced6IXOU1Z"
   },
   "source": [
    "## 1.8 LDA visualization\n",
    "\n",
    "**[10pts]1.8.1** We will now visualize the LDA output using pyLDAvis. PyLDAVis shows the following:\n",
    "\n",
    "   1. The distances between topics, as a map in 2-D space.\n",
    "   2. The variance in the topic-word distribution, as the size of a circle in this map.\n",
    "   3. The most \"salient\" terms in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "iaSJ2dyHOVlM"
   },
   "outputs": [],
   "source": [
    "sentences = read_dataset(\"reviews_Cell_Phones_and_Accessories_5.json.gz\")[:1000] # CHANGE TO YOUR DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5Dn0msKKPJXF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.45 ms, sys: 4.76 ms, total: 14.2 ms\n",
      "Wall time: 14.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uZsFH15wPLo2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaankazancoglu/opt/anaconda3/lib/python3.9/site-packages/pyLDAvis/_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.72 s, sys: 221 ms, total: 1.94 s\n",
      "Wall time: 5.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = None\n",
    "PLDA_dic = corpora.Dictionary(sentences)  # create a gensim dictionary, store it in variable \"dictionary\"\n",
    "PLDA_corp = [PLDA_dic.doc2bow(text) for text in sentences]\n",
    "\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = PLDA_corp,\n",
    "                                            id2word = PLDA_dic,\n",
    "                                            num_topics = 9, \n",
    "                                            alpha = 'auto',\n",
    "                                            eta= 'auto')\n",
    "\n",
    "\n",
    "data = pyLDAvis.gensim.prepare(lda_model, PLDA_corp, PLDA_dic, mds='mmds')\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "D_BnRDcsPNyX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el796661403455920927045201112529\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el796661403455920927045201112529_data = {\"mdsDat\": {\"x\": [-0.01201801470227183, 0.040429493420902504, -0.04010845506091524, 0.006716330204807192, 0.016730906726762675, -0.03047169541689804, -0.04287368853961606, 0.013395570628911335, 0.048199552738317476], \"y\": [0.039064809073810894, 0.018901693553553593, 0.02387255108649467, -0.04933072805475504, 0.04820096992868693, -0.037991721831015034, -0.007876144558100723, -0.004986137468495975, -0.029855291730179322], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [16.272507386929714, 15.8944118807087, 12.922957199088689, 11.58501311072424, 10.383386074756821, 10.252574877098306, 10.040463500740154, 7.557361184567, 5.091324785386374]}, \"tinfo\": {\"Term\": [\"phone\", \"great\", \"headset\", \"keyboard\", \"product\", \"cord\", \"case\", \"usb\", \"price\", \"quality\", \"cable\", \"well\", \"use\", \"adapter\", \"would\", \"one\", \"like\", \"way\", \"charger\", \"buy\", \"better\", \"love\", \"im\", \"time\", \"amazon\", \"long\", \"sound\", \"hub\", \"volume\", \"electronics\", \"safe\", \"lacks\", \"jealous\", \"hdmi\", \"dbm\", \"dock\", \"initiate\", \"loss\", \"sisters\", \"thelg\", \"recycled\", \"faced\", \"bluetoothgreat\", \"ebook\", \"portal\", \"tothere\", \"judy\", \"unitplug\", \"groves\", \"fari\", \"wax\", \"nephew\", \"backnevertheless\", \"medical\", \"greatmake\", \"itgreat\", \"dusty\", \"jobwould\", \"fireworks\", \"usenot\", \"raving\", \"thank\", \"dare\", \"win\", \"phone\", \"frequently\", \"new\", \"wall\", \"much\", \"strength\", \"good\", \"plugged\", \"tone\", \"crackling\", \"signal\", \"brilliant\", \"actually\", \"waste\", \"hbs\", \"everything\", \"travel\", \"pda\", \"phones\", \"purchase\", \"works\", \"reception\", \"price\", \"cable\", \"even\", \"charger\", \"need\", \"get\", \"well\", \"one\", \"headset\", \"like\", \"im\", \"without\", \"service\", \"use\", \"work\", \"battery\", \"great\", \"usb\", \"better\", \"back\", \"would\", \"ear\", \"bluetooth\", \"also\", \"product\", \"bought\", \"quality\", \"searched\", \"wedge\", \"yellowed\", \"lifters\", \"soaked\", \"iphonesi\", \"chevy\", \"mate\", \"percent\", \"issuei\", \"priceyou\", \"unpack\", \"reaching\", \"definetly\", \"prone\", \"specially\", \"boards\", \"batterry\", \"chances\", \"starpluss\", \"horriblei\", \"softer\", \"official\", \"sts\", \"coints\", \"recomendablenote\", \"suburbani\", \"casing\", \"iphonethanks\", \"shapetightness\", \"habit\", \"across\", \"headset\", \"music\", \"replace\", \"yet\", \"system\", \"case\", \"plantronics\", \"sound\", \"logitech\", \"glove\", \"plastic\", \"phone\", \"loved\", \"used\", \"headsets\", \"unit\", \"office\", \"sure\", \"mic\", \"handset\", \"called\", \"transmission\", \"easy\", \"button\", \"nice\", \"pair\", \"use\", \"get\", \"battery\", \"one\", \"time\", \"many\", \"work\", \"like\", \"tried\", \"lot\", \"side\", \"set\", \"good\", \"ear\", \"works\", \"really\", \"also\", \"well\", \"two\", \"even\", \"great\", \"ive\", \"quality\", \"device\", \"bluetooth\", \"would\", \"little\", \"boy\", \"bb\", \"supper\", \"underpowered\", \"prints\", \"antiglare\", \"honda\", \"accord\", \"transmit\", \"waterproof\", \"teamspeak\", \"addicted\", \"gaps\", \"glare\", \"fuses\", \"colorphone\", \"sudden\", \"bubbles\", \"firefighter\", \"burning\", \"bicycle\", \"specified\", \"fussy\", \"pnone\", \"hr\", \"anti\", \"bf\", \"powerd\", \"comfortablethese\", \"quits\", \"transmitter\", \"inserts\", \"auto\", \"offered\", \"earset\", \"compliments\", \"tons\", \"doubles\", \"maxell\", \"walkietalkie\", \"retracted\", \"tooth\", \"collect\", \"models\", \"covers\", \"case\", \"reverse\", \"dust\", \"battery\", \"using\", \"would\", \"far\", \"lg\", \"phone\", \"one\", \"usb\", \"every\", \"bought\", \"great\", \"better\", \"screen\", \"time\", \"clear\", \"cable\", \"sound\", \"get\", \"best\", \"could\", \"works\", \"like\", \"bluetooth\", \"quality\", \"made\", \"used\", \"use\", \"ear\", \"well\", \"im\", \"headset\", \"good\", \"need\", \"ive\", \"really\", \"charger\", \"skill\", \"recepion\", \"voyager\", \"ip\", \"replacedto\", \"late\", \"offline\", \"perfection\", \"tis\", \"wep\", \"awsome\", \"til\", \"machining\", \"allot\", \"omg\", \"upgrated\", \"productsfree\", \"accepted\", \"sirus\", \"rpms\", \"otherthings\", \"fro\", \"hexagonal\", \"minus\", \"defensei\", \"stickers\", \"exspensive\", \"jut\", \"iohone\", \"spongy\", \"repair\", \"truck\", \"ipad\", \"favorite\", \"player\", \"cord\", \"great\", \"mail\", \"keyboard\", \"needed\", \"product\", \"works\", \"cassette\", \"oem\", \"choice\", \"cant\", \"screws\", \"bought\", \"drives\", \"hard\", \"get\", \"drive\", \"intended\", \"never\", \"took\", \"use\", \"one\", \"perfect\", \"thanks\", \"case\", \"perfectly\", \"would\", \"time\", \"work\", \"fit\", \"little\", \"good\", \"got\", \"car\", \"long\", \"phone\", \"like\", \"usb\", \"two\", \"charger\", \"need\", \"price\", \"bluetooth\", \"also\", \"ear\", \"even\", \"sgsii\", \"entertainment\", \"frsgmrscb\", \"ranging\", \"slider\", \"ford\", \"incase\", \"determine\", \"pricesave\", \"kids\", \"shining\", \"radios\", \"whoever\", \"handsi\", \"reel\", \"motorolagiant\", \"million\", \"soomething\", \"describe\", \"casewith\", \"dirt\", \"multitasking\", \"describedfor\", \"scheduled\", \"fatherinlaw\", \"talkabout\", \"counterfiet\", \"eforcity\", \"expedition\", \"anybody\", \"fancy\", \"htc\", \"newest\", \"wiggle\", \"quiet\", \"price\", \"razr\", \"materials\", \"great\", \"quality\", \"screwdrivers\", \"jawbone\", \"wish\", \"info\", \"found\", \"phone\", \"volume\", \"see\", \"capability\", \"company\", \"charge\", \"shelf\", \"ear\", \"motorola\", \"keyboard\", \"buy\", \"one\", \"product\", \"charger\", \"keep\", \"use\", \"sound\", \"little\", \"well\", \"would\", \"work\", \"ive\", \"bluetooth\", \"battery\", \"like\", \"phones\", \"good\", \"headset\", \"works\", \"got\", \"time\", \"get\", \"recomend\", \"este\", \"liion\", \"camper\", \"mikey\", \"mc\", \"paint\", \"nose\", \"walkman\", \"spudger\", \"solvedi\", \"cheddar\", \"lookingthis\", \"builtand\", \"celular\", \"gradually\", \"functionally\", \"siquiera\", \"art\", \"betteru\", \"proclaimed\", \"transactions\", \"chile\", \"hace\", \"receptionon\", \"dios\", \"tapeproblem\", \"itnoteit\", \"whooshing\", \"pice\", \"upset\", \"tab\", \"coffee\", \"fading\", \"ports\", \"first\", \"protector\", \"follow\", \"one\", \"hub\", \"cel\", \"idea\", \"asian\", \"people\", \"exactly\", \"would\", \"worked\", \"tried\", \"purchased\", \"use\", \"item\", \"pieces\", \"keyboard\", \"well\", \"supposed\", \"also\", \"might\", \"like\", \"power\", \"quality\", \"work\", \"problem\", \"could\", \"sound\", \"bluetooth\", \"phone\", \"get\", \"used\", \"problems\", \"headset\", \"bought\", \"good\", \"great\", \"works\", \"im\", \"case\", \"much\", \"ear\", \"time\", \"battery\", \"adaptor\", \"self\", \"interrupting\", \"fuse\", \"usa\", \"vehicles\", \"spigen\", \"habra\", \"descrition\", \"patient\", \"receptionupdate\", \"scuff\", \"setfills\", \"confidence\", \"wrapping\", \"unreliable\", \"yagi\", \"waaaaaaay\", \"six\", \"transmitting\", \"pointrather\", \"changer\", \"ether\", \"coil\", \"worrying\", \"oldfashioned\", \"fannnnntastic\", \"matters\", \"cyberpowerit\", \"atomically\", \"needing\", \"excited\", \"inverter\", \"watts\", \"stopped\", \"cord\", \"aa\", \"unit\", \"love\", \"ac\", \"chargers\", \"plug\", \"charged\", \"like\", \"sabrent\", \"month\", \"charger\", \"even\", \"usb\", \"power\", \"fine\", \"thing\", \"well\", \"battery\", \"charging\", \"good\", \"phone\", \"charges\", \"working\", \"case\", \"seems\", \"use\", \"charge\", \"one\", \"really\", \"im\", \"also\", \"better\", \"hub\", \"bought\", \"ive\", \"works\", \"get\", \"headset\", \"phones\", \"great\", \"quality\", \"sound\", \"neckband\", \"smoking\", \"designthe\", \"distraction\", \"suited\", \"strikesas\", \"drivingon\", \"pinhole\", \"corresponding\", \"loops\", \"evo\", \"learnedcheap\", \"theseit\", \"considerations\", \"iphoe\", \"nervous\", \"preexisting\", \"bedrooms\", \"behaves\", \"workingthats\", \"snapped\", \"latch\", \"csm\", \"mix\", \"hides\", \"shipment\", \"resolved\", \"phonefits\", \"abnormal\", \"levana\", \"sk\", \"defect\", \"lest\", \"symbols\", \"seller\", \"cancel\", \"speaking\", \"baby\", \"background\", \"tunnel\", \"way\", \"tablet\", \"tones\", \"listeners\", \"keyboard\", \"impressedi\", \"bt\", \"searching\", \"mouth\", \"im\", \"keys\", \"iphone\", \"headset\", \"product\", \"excellent\", \"still\", \"function\", \"screen\", \"ive\", \"phone\", \"good\", \"sound\", \"quality\", \"find\", \"noise\", \"really\", \"volume\", \"also\", \"like\", \"better\", \"much\", \"even\", \"bluetooth\", \"battery\", \"get\", \"one\", \"ear\", \"use\", \"great\", \"nice\", \"using\", \"well\", \"would\", \"time\", \"works\", \"whos\", \"terribly\", \"ajoke\", \"bstrds\", \"againaugust\", \"touchpads\", \"angry\", \"moneygobbling\", \"quaility\", \"welli\", \"gasket\", \"cofortable\", \"quickcharge\", \"absoultely\", \"freeze\", \"exceptional\", \"drains\", \"grief\", \"nutshell\", \"dime\", \"timewall\", \"usd\", \"prettyand\", \"callings\", \"amends\", \"staying\", \"ssuper\", \"emits\", \"passengers\", \"problemsi\", \"markings\", \"dose\", \"magnetic\", \"min\", \"closure\", \"sii\", \"tip\", \"wilson\", \"spen\", \"hole\", \"electronics\", \"periodically\", \"clutch\", \"adapter\", \"rubber\", \"told\", \"amazon\", \"phone\", \"open\", \"pry\", \"arrive\", \"buy\", \"meant\", \"use\", \"protecting\", \"cable\", \"product\", \"case\", \"well\", \"better\", \"time\", \"like\", \"one\", \"long\", \"earpiece\", \"ear\", \"would\", \"must\", \"calls\", \"make\", \"without\", \"headset\", \"last\", \"get\", \"phones\", \"back\", \"good\", \"hard\", \"bluetooth\", \"bought\", \"used\", \"really\", \"quality\", \"even\", \"battery\", \"work\", \"great\"], \"Freq\": [916.0, 328.0, 339.0, 121.0, 137.0, 78.0, 220.0, 116.0, 147.0, 183.0, 81.0, 223.0, 317.0, 54.0, 219.0, 429.0, 315.0, 87.0, 142.0, 102.0, 132.0, 67.0, 145.0, 171.0, 44.0, 84.0, 183.0, 66.0, 90.0, 20.0, 2.8391001255014827, 2.1107072279972265, 2.109736942820532, 2.053053501343294, 1.8212768605608427, 3.0051488313809775, 1.8364690707108773, 2.8277528203844065, 1.7382692406551845, 1.59164227894226, 1.1111187004049505, 1.1111235472148244, 1.1111152383978975, 1.1110745021149082, 1.1110816569294844, 1.1111319714319865, 1.1110568458789385, 1.1110493448636571, 1.111021879607704, 1.1110090701816084, 1.1110053773740853, 1.1110198024034723, 1.1110083777801978, 1.11102291820982, 1.1109541396697022, 1.1109696033012053, 1.1108993245580312, 1.1109278284161002, 1.1108896309382832, 1.1108409320390722, 1.1108686280954956, 5.430774761340202, 2.2049012850922365, 2.110617446614321, 269.31530050375517, 2.543933558579138, 31.642223777704643, 5.322343315640349, 49.95810595537919, 6.189965211986173, 77.56337470567577, 12.619611173057288, 6.49359061614112, 4.157084364139226, 19.22833614215363, 2.6811398220999285, 18.30964502576803, 5.05534179169669, 8.884424990821925, 14.55582309036734, 4.626259714357578, 7.822139053900572, 33.32142090741632, 14.56373216087994, 48.10640301503341, 14.534874716490673, 33.74428429688653, 21.6548615015452, 36.7578146473426, 32.17242969625634, 29.673228038406393, 47.617368207556765, 43.68392626782872, 69.20475657644364, 56.734880439631134, 52.37206838357717, 30.770121121030478, 20.16366889564208, 15.910756637879343, 49.70607553473512, 34.080817232081706, 38.34392717781948, 45.715664653348206, 23.926335105060595, 25.764492827411644, 23.862351675511597, 33.21077331560167, 27.731210104468147, 27.834531166156967, 25.41163952936813, 24.225803331146412, 24.892386477929954, 25.173313117442284, 2.372922320621969, 2.3728280876351904, 3.149063298713486, 2.5957193100274916, 1.2496944251304238, 1.249491982015287, 1.249476990403754, 1.2494733833994753, 1.2494100353868323, 1.2494250269983653, 1.249380615758185, 1.249352548756142, 1.2493359790802372, 1.249291567840057, 1.2492904406512197, 1.249274209131966, 1.2492609083036887, 1.2492578648938286, 1.2492221330076936, 1.2492044361429517, 1.2492121010270438, 1.2491203478557067, 1.249153261769749, 1.2490961132957097, 1.24907909274427, 1.249055309059808, 1.2490692862013877, 1.249063762976086, 1.2490225078646493, 1.2490276929333, 1.7881208585900488, 2.3729428354588036, 123.3717668588886, 10.539059623622622, 13.469529670979535, 13.368403699024482, 10.406450816572809, 53.73401886298775, 12.611391791811261, 45.86290790832068, 6.227648889951715, 2.0955772818243603, 16.591036584193294, 155.29111439743724, 5.5213532663978615, 35.698146216938135, 17.323480283513845, 23.10833062297044, 8.570852651446698, 19.09877564935912, 8.779920029941234, 7.38162334078965, 9.536211438049804, 4.066458622235104, 23.764554714879726, 18.71934404827951, 25.38053771924958, 8.599286666174965, 55.18483173093373, 45.85890413357142, 42.623727891023755, 65.52539520385135, 34.24040068676867, 19.620750649022895, 35.06597182605647, 50.45166661148017, 15.73729834849385, 18.192782742675483, 15.969510069942075, 18.24873459304525, 40.91365076953999, 29.217106178165277, 34.375291825775776, 25.207749589789284, 26.692576508082535, 31.190364759833393, 22.700380242562016, 26.829961890548294, 35.770877851211985, 23.90829744238653, 26.02299928633384, 21.574358271381154, 24.16185000464914, 24.62551778014922, 20.718129399227333, 2.6234906608592077, 4.574732776431242, 1.8727582042618172, 1.8638096930072896, 1.8443002485178148, 1.7714239805029215, 1.7726263777055535, 1.7697391580811848, 2.2203541893628937, 2.2276847798693065, 1.5944833505685834, 0.986182540897734, 0.9860494707195159, 0.9860064886853669, 0.9860060304547257, 0.9859879761674606, 0.9859855017219978, 0.9859589243448055, 0.9859624985438072, 0.9859626818360637, 0.9859109934197311, 0.9859358295204866, 0.9858803836128958, 0.9858436335154678, 0.9858296116578456, 0.9858037674496792, 0.9857918534530068, 0.9857856215162859, 0.9857537286636551, 0.985727609517104, 2.385804044445066, 1.339032140300707, 3.154217055396587, 1.8497737218813817, 1.7702045371204351, 2.1706796050523867, 1.76856150533318, 2.4922358091494496, 2.090145386560974, 1.7337356100138968, 1.7393531510910716, 3.3917755505234357, 1.6312127359716064, 5.914927997745562, 4.1888897460433325, 46.70388166182389, 2.7183816119254014, 4.529084939703758, 47.06460375528962, 26.544361124746064, 42.08527458453905, 18.955893076820143, 12.28439439752319, 117.747332688069, 65.78979640005207, 24.1065051954237, 15.809702022751125, 29.363108627439278, 50.10045433728442, 25.782220191519677, 18.1011059727929, 30.496796161631185, 11.566190084517615, 17.259530574614093, 29.09291237995123, 34.65017816516106, 16.4336596569721, 18.461402828223548, 31.535517778223987, 38.54272795600289, 26.27494790177778, 26.829792624594294, 16.156388328383198, 22.309897958631385, 35.38227676437827, 24.328152456348406, 27.991261399519207, 22.000834954737282, 32.88582743283642, 28.39727574273481, 19.591527160949116, 19.739703553778845, 19.059085150890912, 18.965345091903274, 3.6717094984643697, 0.9929587152996258, 0.992922976661192, 0.9929136106731887, 0.9928851019202313, 0.9928872380227584, 0.9928703135181207, 0.9928555251160103, 0.9928601259522224, 0.9928467342325334, 0.9928450910767433, 0.9928288238344218, 0.9928452553923224, 0.9928111599096787, 0.9928077914403091, 0.9928078735980986, 0.9928132960122058, 0.9927674519656632, 0.9927464195715505, 0.9927251407040693, 0.9927234975482793, 0.9926975356867963, 0.9927062444124837, 0.9926503771156218, 0.9926350957667742, 0.9926565389498344, 0.9925922094006537, 0.9926231828872963, 0.9925816932035975, 0.9925760243161217, 4.7607300589310855, 1.431575163380515, 4.511476142194767, 4.8059464199618445, 9.297822013999351, 22.884204397615957, 72.02944556817154, 2.6978210849485538, 29.313263064675446, 14.997564011855578, 31.03503005677689, 44.13781460255983, 5.941368056783949, 8.672526965241367, 2.62834188552183, 19.111922262709633, 3.9381633127486775, 28.49033383832575, 4.620345729483027, 13.38603042634814, 39.26678836848355, 8.403975498275475, 3.825051754472809, 13.638839803582982, 9.20648095541709, 43.93845905496245, 54.34111798980128, 9.729713025431542, 4.635482151989865, 32.77608901237822, 10.525827263812156, 32.05014278433153, 26.714221768886315, 28.1166171144847, 19.321382561153538, 21.911537670430047, 37.30615384739473, 20.380272902529157, 17.71163541462105, 15.561562119759, 67.87014766372178, 34.21860102069735, 18.698901252370447, 18.4894146634339, 20.611216477034567, 19.44187977607455, 20.797106034301276, 21.86847384348426, 19.313604518905727, 18.461879315966463, 16.75779399469065, 1.87733615536898, 1.8372985472763688, 0.9888611451524518, 0.9886897201381254, 0.9887126946245816, 0.9886485575165582, 0.9886066585332456, 0.9885501795873742, 0.9885886176704836, 0.9885330959948813, 0.988542079608175, 0.9885400177952879, 0.988520356936686, 0.9884816979450531, 0.9884670443463199, 0.9884848643005584, 0.9884968669970081, 0.9884836861217657, 0.9884324353442867, 0.988433981703952, 0.9884353807912682, 0.988409534494005, 0.9883888427289597, 0.9884286798993852, 0.9884187390158224, 0.9884068099555472, 0.9883822891094257, 0.9883556328142427, 0.9883224965357001, 0.9883093892966323, 1.8780611771434894, 7.185155766719379, 1.397504285757866, 1.8230645274656605, 3.3893040453664085, 34.54572676302178, 8.998785880230773, 2.155749671312843, 61.63443948291012, 36.80656939354707, 5.11657718791502, 4.765077265111417, 10.431150856466472, 2.2774935368567117, 10.672490178525864, 99.18213517688304, 17.29998034460187, 11.173146652841263, 1.9265223217862977, 4.700442965637595, 18.251577682720413, 2.118012310040476, 23.27590936206713, 16.441556919940215, 18.483951064167957, 16.398866789568366, 43.47832727139825, 19.898928477782146, 20.084291347225296, 12.142149226263205, 33.914204079315944, 23.02611425081309, 18.467601477062747, 25.762081043038876, 24.956911299803718, 21.637892099400545, 17.869245704548, 20.39465898656559, 23.54534235743658, 26.538411325788246, 16.27671203417057, 24.662599881049267, 24.498220375902708, 19.181345724169265, 15.030564107002673, 16.391423055956583, 17.046411526675065, 1.6472813766826966, 1.6468307294195703, 1.6467189037515018, 1.4394778753408555, 1.3860294230969386, 2.336567232973109, 2.4193515278353117, 1.3333519717530204, 0.8672799645743511, 0.867267313295909, 0.8672445555364127, 0.8671645761899397, 0.867133820495796, 0.8671394190500491, 0.8671296761114788, 0.867125386310168, 0.8671357836252095, 0.8671067729349888, 0.8671240048487289, 0.8671168794160431, 0.8670768170343099, 0.8670819793375822, 0.8670774714107811, 0.8670667832617523, 0.8670388631989837, 0.8670366819440799, 0.8670259937950512, 0.8670254848355736, 0.8670470065506245, 0.8670286113009358, 2.031626779502019, 1.6467580209227766, 1.593672238660866, 1.2364209300866909, 8.153346441680458, 21.682176304965463, 5.740452155445381, 1.1131294956163578, 61.40484532781507, 14.766272056917657, 1.1942524740353249, 5.01424143447536, 1.1592175213128704, 14.798470869305685, 6.006446011775021, 29.865072061410793, 13.300585095154625, 11.342303302627084, 11.086294940413083, 35.99189716881708, 11.147958725709703, 4.505972105972098, 17.86670428718023, 27.052076877352214, 4.047609889667926, 20.788901816742975, 7.542319357994825, 32.34951325349464, 14.501542165096277, 21.632725220458262, 21.788353977003016, 13.647659395432608, 13.638571415001389, 19.713776979692106, 19.148541125619072, 49.704124036074255, 22.96095705986708, 16.761336205471093, 10.782622377709078, 25.530096089031744, 16.60756529643641, 23.172935483094808, 24.262646226270434, 17.640663459004656, 14.425992801916012, 16.331071751501806, 14.03959890990803, 14.00911369137243, 13.989461457191096, 13.64163215188241, 3.247217305916138, 1.520815477315029, 1.4873412153735983, 1.4239363851074207, 2.7887037389735254, 1.884832589001518, 1.3974462642222125, 1.2892300297213943, 0.8993532396686273, 0.8992074845482015, 0.899170315924429, 0.8991453944333938, 0.8991584248129922, 0.8991239619511036, 0.8991290886578308, 0.899117909588995, 0.8991000373197098, 0.8990997525026694, 0.8990694194878666, 0.899025842480685, 0.8989862529120691, 0.898968451847044, 0.8989774235838166, 0.8989347722320163, 0.89893028636363, 0.8989092099026403, 0.8989307847934508, 0.8988977460167641, 0.8988103071853606, 0.8987901563797521, 2.638533099970341, 1.7996933703822482, 5.468797056662316, 2.3106326403176145, 6.879928779675855, 23.10111823877535, 2.331421008871011, 19.928632367188104, 16.360209795787974, 4.379468145283534, 7.908341592087495, 15.159154564702455, 7.949587086976299, 47.310835540249016, 2.611041135145518, 7.723906039038722, 25.51120585661397, 29.128945068193822, 21.37170689091683, 18.85134836844793, 15.743425962847217, 19.987252271175524, 30.73813400403041, 31.50368119497772, 12.226780850855885, 34.73300059807499, 71.73242937734604, 8.833591393929131, 9.37889127643566, 23.527585046783027, 11.345752881975256, 28.548082360388594, 15.469964008211766, 33.36842392921743, 16.81277723737811, 17.14294626475764, 17.854874938946995, 15.735672103739308, 11.251599204027256, 16.063366640671823, 15.24550197720711, 18.554117862156833, 17.955235349839437, 19.93947820008662, 14.309143171521626, 18.47682193519616, 15.544819622770275, 15.405459784169636, 1.4221758630473527, 1.3368187990047005, 1.3363604565491691, 1.3324806241075373, 1.3289347942597867, 1.3265101669576065, 1.3216716313068366, 1.3022747200789315, 1.2135884562405936, 0.7485983736660914, 0.7485236089647974, 0.7485219475269909, 0.7485075841291795, 0.7484948821691747, 0.748487700470269, 0.74846176060258, 0.7484499161588624, 0.7484092377299864, 0.7484214037423116, 0.7483405828322389, 0.7483620207394199, 0.7483488364265036, 0.7483240756437095, 0.7483365632246425, 0.748314428585478, 0.7482952416585509, 0.748269141006558, 0.7482398246684878, 0.7481662926468567, 0.7481562704252496, 2.1618834542525387, 1.309753548378123, 1.447159707265719, 1.9458590230655912, 3.6889480298170816, 2.6022836677478947, 2.112977156600341, 1.216993117469557, 4.791043826233353, 1.4812894986352136, 14.370747583421522, 1.7184122605534926, 2.8449674227889457, 1.3196198092105356, 17.42017411523137, 1.4531927631046186, 7.230353522283672, 1.6828000379024264, 3.2913717509485068, 17.898940893692416, 7.291355515925579, 8.122382267539548, 28.605417622252997, 15.733669392588979, 8.20491306508898, 12.207862550145485, 5.49884505861233, 10.300989004425903, 14.040651833744977, 42.14945004595281, 21.702651614505683, 15.9196801105852, 15.376145126948671, 9.53785095975669, 7.511904417423094, 12.521775252449176, 9.687137685235435, 12.948720606639544, 18.687567752483645, 11.773257002961174, 12.413863687829918, 13.15063053411754, 13.280170374533645, 14.828912533081553, 15.08351769392693, 18.83107996328409, 12.408336137842348, 15.895192875486682, 14.582656150808775, 9.820123026093684, 10.038403797011439, 11.611514853410224, 11.137846866793124, 10.367497967664486, 10.35734068724209, 0.9311134498096718, 0.6076591088818915, 0.6076677021819309, 0.6076241579808904, 0.6075903624815755, 0.607608993333762, 0.6075764976613437, 0.6075690597629901, 0.6075479014696156, 0.607507029134974, 0.6075232769711831, 0.607488109432366, 0.6074787940062728, 0.6074928032517153, 0.6074770609037438, 0.6074679621154667, 0.607432866789255, 0.6074179187799426, 0.6074154635513599, 0.6074299060724346, 0.6073679476570238, 0.6073547327502404, 0.607374735641929, 0.6073508332695502, 0.6073374739375561, 0.6073328523308121, 0.6073387737644528, 0.6073415900560624, 0.6073355964098164, 0.6072833144835256, 1.1538176302710406, 1.1542865067177335, 3.039518970369702, 1.1546235229470134, 1.0850819174206758, 1.1375793261258207, 1.5578539453229592, 2.2467071745847433, 1.0967764600105587, 1.415224662070793, 4.283232917669034, 1.227674661094523, 1.016460878187645, 8.156910599707844, 3.5049638740551163, 3.242241995136716, 6.298337224644787, 43.27223334840627, 4.668749443521549, 1.9948979201652373, 1.4294579110152008, 9.463773315004973, 1.6702197922141853, 18.537679439108864, 1.0708259937181805, 7.578327607529335, 9.846191053535682, 13.052230847645507, 12.803690080170435, 9.195129135894646, 10.697258780028722, 14.981259462118093, 17.27527253678244, 6.91109699869466, 3.872721587143659, 9.377240083434154, 10.573707056139868, 4.34341894689355, 4.924018115014098, 5.922030405631239, 5.888917748712665, 11.887726907180815, 4.876631047966054, 10.023741630620616, 7.305963612690989, 6.785163991930995, 9.61516502021842, 5.00979109227554, 7.435282524095426, 6.9622662731617675, 6.665445313135386, 6.398320487242128, 6.687319377854408, 6.573512311785129, 6.9148168144226805, 6.18126845961598, 6.245089382544597], \"Total\": [916.0, 328.0, 339.0, 121.0, 137.0, 78.0, 220.0, 116.0, 147.0, 183.0, 81.0, 223.0, 317.0, 54.0, 219.0, 429.0, 315.0, 87.0, 142.0, 102.0, 132.0, 67.0, 145.0, 171.0, 44.0, 84.0, 183.0, 66.0, 90.0, 20.0, 3.8175366430507744, 2.8455955925417444, 2.844655578638725, 2.8392352653852986, 2.780849786648353, 4.598453403412613, 2.815216243109557, 4.492627413892523, 2.7894456735093036, 2.6351709937137464, 1.8459786526144493, 1.845986986378902, 1.8459954183259544, 1.8459304436892092, 1.8459428700044183, 1.846033067619984, 1.845919037779456, 1.8459266527052618, 1.845885024861718, 1.8458699818453057, 1.8458660244585063, 1.8458923326116956, 1.84587526728603, 1.8458998530073552, 1.8458259511375366, 1.8458585198957338, 1.8457648253742311, 1.845816243604186, 1.8457674033308313, 1.8457053408420876, 1.8457517276493385, 9.708512695609176, 3.839005368174476, 3.734520776115391, 916.2642672376456, 4.697233618583254, 83.2900995708432, 11.167358264673464, 147.65444169937422, 13.44222860989947, 298.06680766228845, 33.948452822510006, 15.307864661527738, 8.951328677736049, 56.747638832680394, 5.2970803843459215, 55.13441131744221, 11.489960015435601, 23.476887103810977, 43.187277392323416, 10.60660095525888, 20.75220041885945, 133.48218294196465, 46.28390603103292, 227.64250508393366, 47.254805688831745, 147.18616455362636, 81.78279011518993, 171.11295933570193, 142.77852934073047, 128.17088576168584, 250.4631021357019, 223.98033443333503, 429.2190151986456, 339.8132973708079, 315.4526513058912, 145.47018255991946, 78.30093816339927, 55.487710307115044, 317.0986990081267, 185.13052336748345, 232.81883676158853, 328.8180955877462, 116.48714188418137, 132.21612448112762, 116.44686150754146, 219.26842246362398, 170.49550088950326, 175.5356716045962, 157.22191425141148, 137.5981629767331, 155.36813400456225, 183.0375015065038, 3.093967544740255, 3.0938877213814573, 4.121755492898127, 4.018010607345186, 1.9707084485869755, 1.9705017911372593, 1.9704926424237423, 1.9704961998501045, 1.970430639000988, 1.970458099988751, 1.970408471216551, 1.9703673236803494, 1.9703496581865758, 1.9703087401263781, 1.9703089536101772, 1.970286289699572, 1.9702684729296605, 1.9702678667466236, 1.9702442555402662, 1.9702232269889774, 1.9702516505164311, 1.9701395758527929, 1.9701916315717352, 1.970114731833964, 1.9700885338104999, 1.970064522865659, 1.9700944594017167, 1.9701042566505755, 1.9700392419991077, 1.9700493037458533, 2.8314306238515043, 3.8661586638998555, 339.8132973708079, 23.116139511029683, 33.567888856475705, 35.958075507066795, 27.854272274122724, 220.8071828664564, 35.56234313398707, 183.80136240460124, 14.834605298039984, 3.7834534543701284, 52.08018238639094, 916.2642672376456, 12.989313918561397, 143.83619986025113, 56.96098774489117, 84.08583950710374, 24.025819102538478, 69.13411310012003, 25.185643780989988, 20.103980671908147, 28.187088853167364, 9.196151566299552, 95.56739855933445, 69.72594766991011, 104.74926887646025, 25.244492624294477, 317.0986990081267, 250.4631021357019, 232.81883676158853, 429.2190151986456, 171.76389961999126, 79.44251789561432, 185.13052336748345, 315.4526513058912, 58.07206874228974, 72.31219820343846, 60.18098289857697, 74.41535240356198, 298.06680766228845, 170.49550088950326, 227.64250508393366, 139.19820571959744, 157.22191425141148, 223.98033443333503, 116.407706153705, 171.11295933570193, 328.8180955877462, 138.77062905244625, 183.0375015065038, 111.62702326021225, 175.5356716045962, 219.26842246362398, 132.7972615633823, 3.506375607244929, 6.173801910657287, 2.6202587545875207, 2.620187473589287, 2.6212264108408774, 2.620736241478932, 2.6227263663814364, 2.6237559432034567, 3.303561475960677, 3.5091251890838606, 2.637519044108404, 1.733551633549564, 1.7334267939513306, 1.733380271984969, 1.7333845288055174, 1.7333653657024473, 1.733363286468064, 1.7333317036625522, 1.7333405382476257, 1.7333416216434263, 1.7332808174758012, 1.7333360310866681, 1.733258997420226, 1.7332257864635219, 1.73320946805556, 1.7331780733820172, 1.7331986015568128, 1.7332011796831361, 1.7331451077681066, 1.733110659244971, 4.447540773685983, 2.415401818973513, 6.272770992878616, 3.4777874431789577, 3.387221245783274, 4.3467676606180055, 3.4335816265070593, 5.1686405399701805, 4.203368559609996, 3.380069785093416, 3.4098806691533072, 7.749866108216488, 3.202758347930716, 15.94624647447549, 10.421332573822312, 220.8071828664564, 6.174368291700108, 11.805892024874806, 232.81883676158853, 117.92052177989787, 219.26842246362398, 78.27462939058881, 44.73777236488412, 916.2642672376456, 429.2190151986456, 116.48714188418137, 65.74747016602669, 155.36813400456225, 328.8180955877462, 132.21612448112762, 81.7376590662425, 171.76389961999126, 45.44653192719791, 81.78279011518993, 183.80136240460124, 250.4631021357019, 80.36069525532365, 97.89378713057907, 227.64250508393366, 315.4526513058912, 175.5356716045962, 183.0375015065038, 81.32064316143678, 143.83619986025113, 317.0986990081267, 170.49550088950326, 223.98033443333503, 145.47018255991946, 339.8132973708079, 298.06680766228845, 128.17088576168584, 138.77062905244625, 139.19820571959744, 142.77852934073047, 4.418459969225963, 1.739661297484591, 1.739613299619887, 1.7396028918524769, 1.73957567716333, 1.7395819811506004, 1.7395614628421567, 1.739550898912644, 1.739562942115334, 1.739551596575747, 1.7395500460780304, 1.7395242871610295, 1.7395635930110283, 1.739506283882449, 1.7395118255962918, 1.7395153972249322, 1.7395302391361098, 1.7394833743493006, 1.7394477190157618, 1.739429283431977, 1.7394308904747957, 1.7394183720974337, 1.7394394980776828, 1.7393490133022562, 1.7393331083312722, 1.7393744832429523, 1.7392991001621132, 1.7393605978266231, 1.7392900856594193, 1.7392909139637156, 8.456005205014572, 2.520723741054149, 8.709927170141208, 10.740727140361294, 23.856209956245912, 78.2223868697372, 328.8180955877462, 5.948058936816568, 121.61822589697168, 53.50698553545534, 137.5981629767331, 227.64250508393366, 17.346982739555177, 29.546663641620945, 6.194347957506901, 88.15377659970872, 10.889509902939373, 155.36813400456225, 13.6935136241793, 58.2113288645679, 250.4631021357019, 31.715709594187583, 10.95985045990831, 62.54837672730029, 36.62109928508215, 317.0986990081267, 429.2190151986456, 39.78781061574481, 14.544566720416157, 220.8071828664564, 45.541417808078826, 219.26842246362398, 171.76389961999126, 185.13052336748345, 109.4805367079291, 132.7972615633823, 298.06680766228845, 122.50603960779486, 101.06885384462427, 84.11809781806195, 916.2642672376456, 315.4526513058912, 116.48714188418137, 116.407706153705, 142.77852934073047, 128.17088576168584, 147.18616455362636, 175.5356716045962, 157.22191425141148, 170.49550088950326, 171.11295933570193, 2.62450106555617, 2.6303681232720812, 1.7359897802081905, 1.735802556654889, 1.7358433504641113, 1.7357630603457594, 1.7357365877248596, 1.7356635074439986, 1.7357442601275361, 1.735657460508719, 1.7356793830163024, 1.7356765147591668, 1.7356487993074698, 1.7356091749029443, 1.7355849128917356, 1.7356230583260277, 1.735646275029451, 1.7356243003212795, 1.7355525080245706, 1.7355776154301525, 1.7355819118907958, 1.7355408437357138, 1.7355105508385809, 1.7355840914731777, 1.7355681122836153, 1.735551650196102, 1.735524349396807, 1.7354861298666464, 1.735444069852662, 1.735430955877569, 3.4054619526337775, 15.813216444993696, 2.590454159775283, 3.6179450645760483, 7.9243406091301525, 147.18616455362636, 27.922664019034784, 4.6278071397159675, 328.8180955877462, 183.0375015065038, 14.571610305063423, 13.917658819526928, 39.57189361348301, 5.218084551409209, 42.20488531983992, 916.2642672376456, 90.35304291799568, 50.021184578321375, 4.368838313996248, 15.841792867513316, 113.03125536890695, 5.19810363713716, 170.49550088950326, 102.32838836527543, 121.61822589697168, 102.12654131123337, 429.2190151986456, 137.5981629767331, 142.77852934073047, 68.14926140277625, 317.0986990081267, 183.80136240460124, 132.7972615633823, 223.98033443333503, 219.26842246362398, 185.13052336748345, 138.77062905244625, 175.5356716045962, 232.81883676158853, 315.4526513058912, 133.48218294196465, 298.06680766228845, 339.8132973708079, 227.64250508393366, 122.50603960779486, 171.76389961999126, 250.4631021357019, 2.4066171633703823, 2.4061518936044948, 2.406027108445122, 2.406994224821179, 2.423363993349589, 4.101056788383712, 4.311969094442479, 2.4178538229838624, 1.626562245910294, 1.6265550171201824, 1.6265278359143676, 1.6264627536858898, 1.6264181475682413, 1.6264332622926, 1.6264188946663167, 1.6264177088581062, 1.6264401052467576, 1.6263977322042322, 1.62643147040696, 1.6264197200046278, 1.6263624162858688, 1.6263751178416166, 1.6263797007561087, 1.6263607736551933, 1.6263307000122769, 1.6263304741521536, 1.6263120587632736, 1.6263114716152312, 1.626355499359059, 1.6263224768940212, 3.9141309334222356, 3.295425836200262, 3.3310399213125352, 2.484813622482389, 27.548361504135382, 96.32105150020368, 18.68763494507336, 2.2466392886807154, 429.2190151986456, 66.68441766492143, 2.4678923976054192, 17.020985944085524, 2.4066197592849843, 79.30462030087621, 23.12997150735283, 219.26842246362398, 72.34705493131791, 58.07206874228974, 58.48988315534433, 317.0986990081267, 59.7122068527142, 16.72625781065293, 121.61822589697168, 223.98033443333503, 14.43802629030048, 157.22191425141148, 35.63743803159548, 315.4526513058912, 98.81557919022747, 183.0375015065038, 185.13052336748345, 90.63499907417766, 97.89378713057907, 183.80136240460124, 175.5356716045962, 916.2642672376456, 250.4631021357019, 143.83619986025113, 67.5431353667198, 339.8132973708079, 155.36813400456225, 298.06680766228845, 328.8180955877462, 227.64250508393366, 145.47018255991946, 220.8071828664564, 147.65444169937422, 170.49550088950326, 171.76389961999126, 232.81883676158853, 5.0017716560425045, 2.537575970114047, 2.547731421105508, 2.492999375939133, 4.886495290414384, 3.336217811050326, 2.49060145734943, 2.3284009706766526, 1.6554383968542858, 1.6552829421206317, 1.6552470283126313, 1.6552166668424602, 1.6552490205966544, 1.6552068516298881, 1.6552180094013116, 1.6552034464779013, 1.6551739264313476, 1.6551967562769705, 1.6551650083302112, 1.6551135384637867, 1.6550800448135112, 1.6550513507352982, 1.6550751996728998, 1.655016733352643, 1.6550095697343026, 1.65498470885154, 1.655036063233142, 1.6549827498323662, 1.6548834031628779, 1.6548753412195862, 4.915923355507051, 3.3468150423906433, 12.667063749555481, 5.003174511288098, 18.080508690370586, 78.2223868697372, 5.1814791020937685, 84.08583950710374, 67.28666875142595, 12.829475937931381, 27.6847750429495, 67.87878859707489, 29.21718021538649, 315.4526513058912, 6.7630115817846335, 28.97136036669756, 142.77852934073047, 171.11295933570193, 116.48714188418137, 98.81557919022747, 78.66360761036954, 112.1460200284447, 223.98033443333503, 232.81883676158853, 60.79353794115679, 298.06680766228845, 916.2642672376456, 40.10301247818416, 45.078817730413355, 220.8071828664564, 66.97708497743102, 317.0986990081267, 113.03125536890695, 429.2190151986456, 139.19820571959744, 145.47018255991946, 157.22191425141148, 132.21612448112762, 66.68441766492143, 155.36813400456225, 138.77062905244625, 227.64250508393366, 250.4631021357019, 339.8132973708079, 133.48218294196465, 328.8180955877462, 183.0375015065038, 183.80136240460124, 2.1933637557455796, 2.2079850099249865, 2.207700133840792, 2.2084732140902377, 2.2090388956110654, 2.209787367349021, 2.210538742080311, 2.212887542409085, 2.2693136255466944, 1.5197621789697409, 1.5196913263007448, 1.5197033438235088, 1.5196769118434343, 1.5196585497833575, 1.519653658390995, 1.5196292353781669, 1.5196145710566638, 1.51957114824885, 1.5195976894568923, 1.5195122077169003, 1.5195583642910098, 1.5195334976478616, 1.519491256095675, 1.5195368976821038, 1.519496691133444, 1.5194909229924978, 1.519473303416676, 1.5194288243224732, 1.5193488339198609, 1.5193285538798382, 4.455126110708902, 2.7586244334812684, 3.127754248030085, 4.684435465061975, 10.630739553939303, 6.954499657992898, 5.573103592374649, 2.8451754416649244, 17.99301039244717, 3.8116208129934255, 87.98600034703804, 4.802410652075207, 9.690890706679406, 3.334357842817214, 121.61822589697168, 3.822330213259599, 36.634487200638155, 4.804891821055652, 12.584693285300466, 145.47018255991946, 39.622989648808414, 49.07468422306028, 339.8132973708079, 137.5981629767331, 53.26378758845345, 105.88096301950259, 29.9996759550683, 81.7376590662425, 138.77062905244625, 916.2642672376456, 298.06680766228845, 183.80136240460124, 183.0375015065038, 81.08720869014847, 53.8664688679477, 139.19820571959744, 90.35304291799568, 157.22191425141148, 315.4526513058912, 132.21612448112762, 147.65444169937422, 171.11295933570193, 175.5356716045962, 232.81883676158853, 250.4631021357019, 429.2190151986456, 170.49550088950326, 317.0986990081267, 328.8180955877462, 104.74926887646025, 117.92052177989787, 223.98033443333503, 219.26842246362398, 171.76389961999126, 227.64250508393366, 2.073454127723454, 1.3929433623229746, 1.3929657027263338, 1.3929036253058298, 1.3928620096475677, 1.3929088280685127, 1.3928454012508718, 1.3928490197447507, 1.3928500342220471, 1.3927783942612173, 1.3928284136628701, 1.392770835348291, 1.3927498248694197, 1.3927834775077366, 1.3927480761848203, 1.3927520756170484, 1.3927062620285122, 1.3926879456733108, 1.392689799154756, 1.392738890246636, 1.3926404169476592, 1.3926273841284753, 1.3926899098656034, 1.3926368141404493, 1.3926119085038007, 1.3926026019028446, 1.392626081079081, 1.3926374849571392, 1.3926317841823757, 1.3925564460451478, 2.754057128609417, 2.851009261902104, 8.543530455001878, 2.9529588061064276, 2.766744141366825, 2.930912824729007, 4.369546279449274, 7.081623985177499, 2.931693236267498, 4.32956399923416, 20.74346989783306, 3.6899448684650262, 2.842855806219732, 54.97070770489522, 16.589885972438914, 15.789144546699506, 44.920315941537424, 916.2642672376456, 31.087043928367205, 8.614335395371544, 5.194978973436508, 102.12654131123337, 6.852281128445087, 317.0986990081267, 3.463164415499829, 81.78279011518993, 137.5981629767331, 220.8071828664564, 223.98033443333503, 132.21612448112762, 171.76389961999126, 315.4526513058912, 429.2190151986456, 84.11809781806195, 31.58950197751095, 170.49550088950326, 219.26842246362398, 40.809871753359886, 53.08371426597482, 78.04957870107407, 78.30093816339927, 339.8132973708079, 54.25839942140901, 250.4631021357019, 133.48218294196465, 116.44686150754146, 298.06680766228845, 58.2113288645679, 175.5356716045962, 155.36813400456225, 143.83619986025113, 139.19820571959744, 183.0375015065038, 171.11295933570193, 232.81883676158853, 185.13052336748345, 328.8180955877462], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.935, -8.2314, -8.2319, -8.2591, -8.3789, -7.8781, -8.3706, -7.939, -8.4255, -8.5137, -8.8731, -8.8731, -8.8731, -8.8731, -8.8731, -8.8731, -8.8731, -8.8731, -8.8732, -8.8732, -8.8732, -8.8732, -8.8732, -8.8732, -8.8732, -8.8732, -8.8733, -8.8732, -8.8733, -8.8733, -8.8733, -7.2864, -8.1878, -8.2315, -3.3826, -8.0447, -5.5239, -7.3065, -5.0673, -7.1555, -4.6273, -6.4432, -7.1076, -7.5536, -6.0221, -7.9922, -6.071, -7.358, -6.7941, -6.3004, -7.4467, -6.9215, -5.4722, -6.2999, -5.105, -6.3019, -5.4596, -5.9032, -5.3741, -5.5073, -5.5882, -5.1152, -5.2015, -4.7414, -4.94, -5.0201, -5.5519, -5.9746, -6.2114, -5.0723, -5.4497, -5.3318, -5.156, -5.8035, -5.7294, -5.8061, -5.4756, -5.6559, -5.6522, -5.7432, -5.791, -5.7639, -5.7527, -8.0908, -8.0908, -7.8078, -8.0011, -8.732, -8.7322, -8.7322, -8.7322, -8.7323, -8.7322, -8.7323, -8.7323, -8.7323, -8.7324, -8.7324, -8.7324, -8.7324, -8.7324, -8.7324, -8.7324, -8.7324, -8.7325, -8.7325, -8.7325, -8.7325, -8.7325, -8.7325, -8.7325, -8.7326, -8.7326, -8.3738, -8.0908, -4.1397, -6.5998, -6.3545, -6.362, -6.6125, -4.9709, -6.4203, -5.1293, -7.1259, -8.2151, -6.1461, -3.9096, -7.2463, -5.3798, -6.1029, -5.8147, -6.8066, -6.0053, -6.7825, -6.9559, -6.6998, -7.5522, -5.7867, -6.0254, -5.7209, -6.8033, -4.9442, -5.1294, -5.2025, -4.7725, -5.4215, -5.9783, -5.3977, -5.0339, -6.1989, -6.0539, -6.1842, -6.0508, -5.2435, -5.5802, -5.4176, -5.7278, -5.6705, -5.5148, -5.8325, -5.6654, -5.3778, -5.7807, -5.6959, -5.8834, -5.7702, -5.7511, -5.9239, -7.7835, -7.2274, -8.1206, -8.1253, -8.1359, -8.1762, -8.1755, -8.1771, -7.9503, -7.947, -8.2814, -8.7619, -8.762, -8.7621, -8.7621, -8.7621, -8.7621, -8.7621, -8.7621, -8.7621, -8.7622, -8.7621, -8.7622, -8.7622, -8.7622, -8.7623, -8.7623, -8.7623, -8.7623, -8.7623, -7.8784, -8.456, -7.5992, -8.1329, -8.1769, -7.9729, -8.1778, -7.8348, -8.0107, -8.1977, -8.1945, -7.5266, -8.2586, -6.9705, -7.3155, -4.9041, -7.7479, -7.2374, -4.8964, -5.4692, -5.0083, -5.8059, -6.2396, -3.9794, -4.5615, -5.5655, -5.9873, -5.3682, -4.8339, -5.4983, -5.852, -5.3303, -6.2999, -5.8996, -5.3775, -5.2027, -5.9486, -5.8323, -5.2969, -5.0962, -5.4794, -5.4585, -5.9657, -5.6429, -5.1818, -5.5563, -5.4161, -5.6569, -5.2549, -5.4017, -5.7729, -5.7653, -5.8004, -5.8054, -7.338, -8.6457, -8.6458, -8.6458, -8.6458, -8.6458, -8.6458, -8.6458, -8.6458, -8.6459, -8.6459, -8.6459, -8.6459, -8.6459, -8.6459, -8.6459, -8.6459, -8.6459, -8.646, -8.646, -8.646, -8.646, -8.646, -8.6461, -8.6461, -8.646, -8.6461, -8.6461, -8.6461, -8.6461, -7.0783, -8.2799, -7.132, -7.0688, -6.4089, -5.5082, -4.3616, -7.6462, -5.2606, -5.9308, -5.2036, -4.8514, -6.8567, -6.4785, -7.6723, -5.6884, -7.268, -5.2891, -7.1082, -6.0445, -4.9683, -6.51, -7.2971, -6.0258, -6.4188, -4.8559, -4.6434, -6.3635, -7.1049, -5.149, -6.2848, -5.1714, -5.3535, -5.3023, -5.6775, -5.5517, -5.0195, -5.6241, -5.7645, -5.8939, -4.4211, -5.1059, -5.7102, -5.7215, -5.6128, -5.6712, -5.6039, -5.5536, -5.6779, -5.723, -5.8198, -7.8993, -7.9209, -8.5404, -8.5405, -8.5405, -8.5406, -8.5406, -8.5407, -8.5406, -8.5407, -8.5407, -8.5407, -8.5407, -8.5408, -8.5408, -8.5408, -8.5407, -8.5408, -8.5408, -8.5408, -8.5408, -8.5408, -8.5408, -8.5408, -8.5408, -8.5408, -8.5409, -8.5409, -8.5409, -8.5409, -7.8989, -6.5572, -8.1945, -7.9287, -7.3085, -4.9869, -6.3321, -7.761, -4.4079, -4.9235, -6.8967, -6.9679, -6.1844, -7.7061, -6.1615, -3.9322, -5.6785, -6.1157, -7.8735, -6.9815, -5.6249, -7.7787, -5.3818, -5.7294, -5.6123, -5.732, -4.7569, -5.5385, -5.5292, -6.0325, -5.0053, -5.3925, -5.6132, -5.2803, -5.312, -5.4547, -5.6461, -5.5139, -5.3702, -5.2506, -5.7394, -5.3239, -5.3306, -5.5752, -5.8191, -5.7324, -5.6932, -8.0174, -8.0176, -8.0177, -8.1522, -8.19, -7.6678, -7.633, -8.2288, -8.6589, -8.6589, -8.6589, -8.659, -8.6591, -8.659, -8.6591, -8.6591, -8.6591, -8.6591, -8.6591, -8.6591, -8.6591, -8.6591, -8.6591, -8.6591, -8.6592, -8.6592, -8.6592, -8.6592, -8.6592, -8.6592, -7.8077, -8.0177, -8.0504, -8.3043, -6.4181, -5.44, -6.769, -8.4093, -4.399, -5.8241, -8.339, -6.9042, -8.3687, -5.822, -6.7237, -5.1198, -5.9287, -6.088, -6.1108, -4.9332, -6.1052, -7.0111, -5.6336, -5.2187, -7.1184, -5.4821, -6.496, -5.0399, -5.8422, -5.4423, -5.4351, -5.9029, -5.9036, -5.5352, -5.5643, -4.6104, -5.3827, -5.6974, -6.1386, -5.2766, -5.7066, -5.3735, -5.3276, -5.6463, -5.8475, -5.7234, -5.8746, -5.8768, -5.8782, -5.9034, -7.3178, -8.0763, -8.0986, -8.1422, -7.47, -7.8617, -8.1609, -8.2415, -8.6017, -8.6018, -8.6019, -8.6019, -8.6019, -8.6019, -8.6019, -8.6019, -8.6019, -8.6019, -8.602, -8.602, -8.6021, -8.6021, -8.6021, -8.6021, -8.6021, -8.6022, -8.6021, -8.6022, -8.6023, -8.6023, -7.5254, -7.908, -6.7965, -7.6581, -6.567, -5.3557, -7.6491, -5.5034, -5.7007, -7.0187, -6.4277, -5.777, -6.4225, -4.6388, -7.5358, -6.4513, -5.2565, -5.1239, -5.4335, -5.559, -5.7392, -5.5005, -5.0701, -5.0455, -5.992, -4.9479, -4.2226, -6.317, -6.2571, -5.3374, -6.0667, -5.144, -5.7567, -4.988, -5.6734, -5.654, -5.6133, -5.7397, -6.0751, -5.719, -5.7713, -5.5749, -5.6077, -5.5029, -5.8347, -5.5791, -5.7519, -5.7609, -7.8593, -7.9212, -7.9215, -7.9244, -7.9271, -7.9289, -7.9326, -7.9474, -8.0179, -8.501, -8.5011, -8.5011, -8.5012, -8.5012, -8.5012, -8.5012, -8.5012, -8.5013, -8.5013, -8.5014, -8.5014, -8.5014, -8.5014, -8.5014, -8.5014, -8.5014, -8.5015, -8.5015, -8.5016, -8.5016, -7.4405, -7.9416, -7.8419, -7.5458, -6.9061, -7.2551, -7.4634, -8.0151, -6.6447, -7.8186, -5.5463, -7.6701, -7.1659, -7.9341, -5.3539, -7.8377, -6.2332, -7.691, -7.0202, -5.3267, -6.2248, -6.1169, -4.8579, -5.4557, -6.1068, -5.7094, -6.5069, -5.8792, -5.5695, -4.4703, -5.134, -5.4439, -5.4787, -5.9562, -6.195, -5.684, -5.9407, -5.6505, -5.2836, -5.7457, -5.6927, -5.635, -5.6252, -5.5149, -5.4979, -5.276, -5.6931, -5.4455, -5.5317, -5.9271, -5.9051, -5.7595, -5.8011, -5.8728, -5.8738, -7.8879, -8.3146, -8.3146, -8.3147, -8.3148, -8.3147, -8.3148, -8.3148, -8.3148, -8.3149, -8.3149, -8.3149, -8.3149, -8.3149, -8.3149, -8.315, -8.315, -8.315, -8.315, -8.315, -8.3151, -8.3151, -8.3151, -8.3151, -8.3152, -8.3152, -8.3152, -8.3152, -8.3152, -8.3153, -7.6734, -7.673, -6.7048, -7.6727, -7.7348, -7.6876, -7.3732, -7.007, -7.7241, -7.4692, -6.3618, -7.6114, -7.8002, -5.7176, -6.5623, -6.6402, -5.9762, -4.049, -6.2756, -7.1259, -7.4592, -5.569, -7.3035, -4.8967, -7.7481, -5.7912, -5.5294, -5.2475, -5.2668, -5.5978, -5.4465, -5.1097, -4.9672, -5.8834, -6.4625, -5.5782, -5.4581, -6.3478, -6.2224, -6.0378, -6.0434, -5.341, -6.232, -5.5115, -5.8278, -5.9018, -5.5532, -6.2051, -5.8103, -5.876, -5.9196, -5.9605, -5.9163, -5.9335, -5.8828, -5.995, -5.9847], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.5196, 1.5169, 1.5168, 1.4915, 1.3925, 1.3903, 1.3885, 1.3527, 1.3427, 1.3115, 1.3081, 1.3081, 1.308, 1.308, 1.308, 1.308, 1.308, 1.308, 1.308, 1.308, 1.308, 1.308, 1.308, 1.308, 1.308, 1.308, 1.308, 1.308, 1.308, 1.3079, 1.3079, 1.2348, 1.2612, 1.2451, 0.5913, 1.2024, 0.8479, 1.0746, 0.732, 1.0402, 0.4695, 0.8261, 0.9581, 1.0487, 0.7335, 1.1348, 0.7133, 0.9947, 0.844, 0.7281, 0.986, 0.84, 0.4279, 0.6594, 0.2613, 0.6367, 0.3428, 0.4869, 0.2777, 0.3255, 0.3526, 0.1556, 0.1811, -0.0092, 0.0257, 0.0201, 0.2623, 0.459, 0.5665, -0.0374, 0.1234, 0.012, -0.1574, 0.2329, 0.1803, 0.2306, -0.0717, -0.0005, -0.0259, -0.0068, 0.0788, -0.0155, -0.1682, 1.5739, 1.5739, 1.57, 1.4023, 1.3837, 1.3837, 1.3836, 1.3836, 1.3836, 1.3836, 1.3836, 1.3836, 1.3836, 1.3836, 1.3836, 1.3836, 1.3836, 1.3836, 1.3836, 1.3836, 1.3836, 1.3835, 1.3835, 1.3835, 1.3835, 1.3835, 1.3835, 1.3835, 1.3835, 1.3835, 1.3796, 1.3511, 0.826, 1.0538, 0.9261, 0.8497, 0.8546, 0.426, 0.8025, 0.451, 0.9712, 1.2484, 0.6953, 0.0642, 0.9837, 0.4456, 0.6489, 0.5476, 0.8084, 0.5528, 0.7854, 0.8373, 0.7554, 1.0232, 0.4476, 0.5242, 0.4216, 0.7623, 0.0907, 0.1415, 0.1414, -0.0403, 0.2265, 0.4408, 0.1754, 0.0062, 0.5336, 0.4592, 0.5125, 0.4336, -0.1467, 0.0752, -0.0512, 0.1305, 0.0659, -0.1322, 0.2045, -0.0136, -0.3792, 0.0806, -0.1115, 0.1955, -0.1439, -0.3473, -0.0186, 1.7561, 1.7464, 1.7103, 1.7055, 1.6946, 1.6545, 1.6544, 1.6524, 1.6488, 1.5918, 1.5429, 1.4821, 1.482, 1.482, 1.482, 1.482, 1.482, 1.482, 1.482, 1.482, 1.482, 1.482, 1.4819, 1.4819, 1.4819, 1.4819, 1.4819, 1.4819, 1.4819, 1.4819, 1.4233, 1.4562, 1.3587, 1.4148, 1.3973, 1.3518, 1.3827, 1.3167, 1.3475, 1.3785, 1.373, 1.2198, 1.3715, 1.0544, 1.1347, 0.4927, 1.2258, 1.0881, 0.4474, 0.555, 0.3956, 0.6281, 0.7537, -0.0056, 0.1707, 0.4709, 0.621, 0.3801, 0.1647, 0.4114, 0.5386, 0.3177, 0.6777, 0.4905, 0.2028, 0.0682, 0.459, 0.378, 0.0695, -0.0561, 0.1469, 0.126, 0.4301, 0.1825, -0.1468, 0.0991, -0.0335, 0.1573, -0.2892, -0.3049, 0.1679, 0.096, 0.0578, 0.0275, 1.9703, 1.5947, 1.5947, 1.5947, 1.5947, 1.5947, 1.5947, 1.5947, 1.5947, 1.5947, 1.5947, 1.5946, 1.5946, 1.5946, 1.5946, 1.5946, 1.5946, 1.5946, 1.5946, 1.5946, 1.5946, 1.5946, 1.5946, 1.5946, 1.5946, 1.5946, 1.5945, 1.5945, 1.5945, 1.5945, 1.581, 1.5897, 1.4976, 1.3513, 1.2132, 0.9263, 0.637, 1.3648, 0.7326, 0.8835, 0.6662, 0.515, 1.084, 0.9296, 1.2982, 0.6267, 1.1384, 0.4592, 1.069, 0.6856, 0.3025, 0.8274, 1.1028, 0.6324, 0.7747, 0.179, 0.0888, 0.7471, 1.012, 0.2479, 0.6907, 0.2325, 0.2945, 0.2708, 0.4209, 0.3536, 0.0773, 0.3619, 0.4139, 0.468, -0.4473, -0.0658, 0.3261, 0.3156, 0.22, 0.2695, 0.1986, 0.0727, 0.0586, -0.0675, -0.168, 1.9299, 1.9061, 1.7022, 1.7021, 1.7021, 1.7021, 1.7021, 1.7021, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.702, 1.7019, 1.6698, 1.4761, 1.6478, 1.5796, 1.4156, 0.8155, 1.1326, 1.501, 0.5907, 0.6609, 1.2184, 1.1931, 0.9316, 1.4359, 0.8901, 0.0416, 0.6119, 0.766, 1.4462, 1.05, 0.4416, 1.3671, 0.2737, 0.4366, 0.381, 0.436, -0.0247, 0.3313, 0.3036, 0.5399, 0.0296, 0.1877, 0.2922, 0.1023, 0.0918, 0.1183, 0.2152, 0.1124, -0.0264, -0.2105, 0.1607, -0.2271, -0.3648, -0.2089, 0.1669, -0.0844, -0.4224, 1.8985, 1.8985, 1.8984, 1.7635, 1.7189, 1.7151, 1.6997, 1.6825, 1.6488, 1.6488, 1.6488, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6486, 1.6486, 1.6486, 1.6486, 1.6486, 1.6486, 1.6219, 1.5839, 1.5404, 1.5797, 1.0601, 0.7864, 1.0973, 1.5754, 0.3332, 0.77, 1.5518, 1.0555, 1.5472, 0.5989, 0.9293, 0.284, 0.584, 0.6445, 0.6145, 0.1017, 0.5994, 0.9661, 0.3597, 0.1638, 1.0059, 0.2544, 0.7248, 0.0002, 0.3586, 0.1422, 0.138, 0.3844, 0.3067, 0.0451, 0.062, -0.6366, -0.1119, 0.128, 0.4428, -0.3109, 0.0417, -0.2767, -0.3289, -0.2799, -0.0333, -0.3266, -0.0754, -0.2214, -0.2302, -0.5595, 1.8666, 1.7866, 1.7603, 1.7385, 1.7376, 1.7275, 1.7207, 1.7074, 1.6884, 1.6883, 1.6883, 1.6883, 1.6883, 1.6883, 1.6883, 1.6883, 1.6883, 1.6883, 1.6883, 1.6882, 1.6882, 1.6882, 1.6882, 1.6882, 1.6882, 1.6882, 1.6882, 1.6882, 1.6881, 1.6881, 1.6763, 1.6782, 1.4586, 1.526, 1.3323, 1.0789, 1.4999, 0.8589, 0.8844, 1.2237, 1.0456, 0.7994, 0.9969, 0.4013, 1.3468, 0.9766, 0.5764, 0.528, 0.6028, 0.6419, 0.6898, 0.5738, 0.3125, 0.2984, 0.6947, 0.1489, -0.2488, 0.7857, 0.7286, 0.0594, 0.523, -0.1091, 0.3098, -0.2558, 0.1848, 0.1602, 0.1232, 0.17, 0.5191, 0.0293, 0.09, -0.2085, -0.3369, -0.5371, 0.0655, -0.5804, -0.1674, -0.1806, 2.1494, 2.0809, 2.0806, 2.0774, 2.0745, 2.0723, 2.0683, 2.0525, 1.9568, 1.8745, 1.8745, 1.8745, 1.8745, 1.8745, 1.8745, 1.8744, 1.8744, 1.8744, 1.8744, 1.8744, 1.8744, 1.8744, 1.8744, 1.8743, 1.8743, 1.8743, 1.8743, 1.8743, 1.8742, 1.8742, 1.8596, 1.8378, 1.8119, 1.7041, 1.5242, 1.5996, 1.6128, 1.7334, 1.2594, 1.6375, 0.7707, 1.5549, 1.357, 1.6557, 0.6394, 1.6156, 0.9599, 1.5335, 1.2415, 0.4874, 0.8899, 0.7839, 0.1078, 0.4141, 0.7121, 0.4224, 0.886, 0.5114, 0.2918, -0.4964, -0.0372, 0.1363, 0.1058, 0.4424, 0.6126, 0.1742, 0.3497, 0.086, -0.2435, 0.164, 0.1066, 0.0168, 0.0011, -0.171, -0.2271, -0.5438, -0.0377, -0.4105, -0.533, 0.2155, 0.1191, -0.3769, -0.3973, -0.2248, -0.5074, 2.177, 2.1481, 2.1481, 2.148, 2.148, 2.148, 2.148, 2.148, 2.148, 2.1479, 2.1479, 2.1479, 2.1479, 2.1479, 2.1479, 2.1479, 2.1479, 2.1479, 2.1479, 2.1478, 2.1478, 2.1478, 2.1478, 2.1478, 2.1478, 2.1478, 2.1478, 2.1478, 2.1478, 2.1477, 2.1076, 2.0734, 1.9442, 2.0386, 2.0416, 2.0312, 1.9463, 1.8296, 1.9944, 1.8595, 1.4001, 1.8771, 1.9491, 1.0697, 1.423, 1.3946, 1.013, -0.0752, 1.0817, 1.5148, 1.6872, 0.5989, 1.566, 0.1382, 1.8039, 0.5989, 0.3404, 0.1493, 0.1158, 0.3119, 0.2015, -0.0696, -0.2351, 0.4785, 0.8788, 0.0772, -0.0543, 0.7374, 0.5999, 0.399, 0.3901, -0.3753, 0.5683, -0.2407, 0.0724, 0.1349, -0.4563, 0.5249, -0.184, -0.1277, -0.0941, -0.1022, -0.3318, -0.2816, -0.539, -0.4219, -0.9861]}, \"token.table\": {\"Topic\": [1, 3, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 4, 3, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 7, 3, 9, 9, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 9, 3, 3, 5, 1, 4, 6, 9, 6, 6, 7, 1, 2, 3, 4, 4, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 8, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 3, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 1, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 6, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 1, 2, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 2, 1, 2, 3, 4, 6, 7, 8, 9, 6, 6, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 2, 6, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 9, 7, 9, 6, 9, 7, 2, 3, 3, 3, 1, 2, 3, 4, 5, 6, 7, 8, 3, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 3, 4, 5, 6, 8, 1, 2, 3, 4, 7, 8, 7, 1, 2, 1, 8, 9, 4, 2, 5, 5, 7, 8, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 6, 5, 8, 1, 6, 3, 9, 3, 4, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 8, 1, 2, 3, 6, 8, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 5, 1, 3, 4, 5, 6, 7, 8, 9, 9, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 7, 5, 4, 1, 6, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 5, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 7, 3, 3, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 9, 1, 2, 7, 6, 1, 2, 3, 4, 5, 6, 7, 8, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 7, 8, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 8, 3, 4, 9, 3, 2, 3, 1, 2, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 7, 8, 5, 5, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 7, 1, 3, 5, 6, 7, 4, 4, 1, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 2, 2, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 1, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 4, 8, 2, 8, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 8, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 5, 6, 7, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 6, 7, 9, 1, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 9, 2, 2, 3, 5, 7, 3, 8, 3, 5, 6, 2, 3, 6, 7, 8, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 5, 2, 9, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 3, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 6, 7, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 4, 7, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 2, 6, 1, 2, 4, 5, 6, 7, 8, 9, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 2, 8, 9, 1, 2, 3, 4, 6, 8, 9, 1, 2, 3, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 1, 2, 3, 4, 5, 8, 3, 5, 5, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 7, 6, 2, 1, 5, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 8, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 5, 6, 7, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 5, 7, 9, 7, 2, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 5, 2, 3, 4, 5, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 9, 6, 4, 1, 7, 6, 7, 8, 4, 5, 8, 8, 2, 2, 6, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 6, 8, 2, 3, 1, 9, 7, 4, 6, 9, 2, 9, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 7, 8, 1, 2, 3, 5, 6, 7, 8, 8, 2, 2, 3, 8, 3, 1, 2, 3, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 6, 4, 5, 8, 5, 6, 3, 9, 1, 3, 6, 8, 2, 3, 4, 5, 6, 7, 8, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 2, 6, 9, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 4, 5, 6, 1, 9, 6, 1, 2, 3, 7, 8, 3, 1, 3, 4, 7, 1, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 7, 4, 6, 8, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 7, 3, 7, 6, 1, 3, 4, 7, 9, 1, 2, 3, 6, 7, 8, 3, 5, 3, 6, 7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 4, 5, 6, 9, 1, 5, 2, 6, 7, 8, 9, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 7, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9], \"Freq\": [0.19299508505127677, 0.19299508505127677, 0.38599017010255354, 0.6581766989085969, 0.7179866907880125, 0.0779455064912994, 0.0779455064912994, 0.1558910129825988, 0.0779455064912994, 0.1558910129825988, 0.0779455064912994, 0.3117820259651976, 0.5748833330321862, 0.762266019894409, 0.517309343425282, 0.258654671712641, 0.32647487421898996, 0.09068746506083054, 0.07254997204866442, 0.054412479036498325, 0.09068746506083054, 0.16323743710949498, 0.14509994409732885, 0.03627498602433221, 0.018137493012166106, 0.20010657419679598, 0.09095753372581636, 0.07276602698065308, 0.14553205396130617, 0.054574520235489817, 0.07276602698065308, 0.14553205396130617, 0.07276602698065308, 0.14553205396130617, 0.19992915885952672, 0.5997874765785802, 0.576850426977149, 0.7179462093685989, 0.7178927650858773, 0.574875761740897, 0.15901091218125504, 0.17173178515575543, 0.11448785677050363, 0.12084829325775383, 0.06996480135975222, 0.13356916623225423, 0.11448785677050363, 0.08268567433425261, 0.031802182436251006, 0.20035477959935225, 0.08904656871082323, 0.08904656871082323, 0.04452328435541161, 0.08904656871082323, 0.17809313742164645, 0.08904656871082323, 0.08904656871082323, 0.13356985306623484, 0.7180751463445286, 0.717954770214936, 0.5769747583112804, 0.7631443288132503, 0.5762257476237781, 0.19249356063100564, 0.19249356063100564, 0.19249356063100564, 0.19249356063100564, 0.6148429971966686, 0.41552056411981914, 0.6042751227793596, 0.15941917872265465, 0.15941917872265465, 0.47825753616796396, 0.15941917872265465, 0.5748612994806264, 0.3514721747404179, 0.3514721747404179, 0.20610259211190235, 0.16316455208858938, 0.13740172807460158, 0.08587608004662599, 0.10305129605595117, 0.11163890406061378, 0.06870086403730079, 0.06870086403730079, 0.06011325603263819, 0.11115427359723691, 0.16673141039585537, 0.055577136798618455, 0.055577136798618455, 0.11115427359723691, 0.11115427359723691, 0.11115427359723691, 0.27788568399309227, 0.055577136798618455, 0.5417484148158553, 0.5075452007707133, 0.16321703401908502, 0.184692959547912, 0.2018736999709736, 0.060132591480715536, 0.1030844425383695, 0.060132591480715536, 0.13744592338449266, 0.06442777658648094, 0.030066295740357768, 0.8098737329697837, 0.6580804071940939, 0.6580689131985995, 0.14932673195364168, 0.22399009793046254, 0.19910230927152225, 0.07466336597682084, 0.09955115463576113, 0.062219471647350706, 0.049775577317880564, 0.08710726030629098, 0.049775577317880564, 0.1966477243379737, 0.10588723618198584, 0.1966477243379737, 0.09832386216898685, 0.06050699210399191, 0.0680703661169909, 0.12101398420798382, 0.09076048815598786, 0.0680703661169909, 0.614847439255812, 0.5769679245654646, 0.5769405568431275, 0.15951173766590046, 0.13672434657077182, 0.14811804211833615, 0.1253306510232075, 0.11393695547564318, 0.10824010770186103, 0.08545271660673238, 0.07405902105916808, 0.039877934416475115, 0.5417131538207459, 0.5075450446167193, 0.16090815636149622, 0.10298122007135758, 0.1866534613793356, 0.18021713512487575, 0.07079958879905833, 0.10941754632581743, 0.10298122007135758, 0.03218163127229924, 0.04505428378121894, 0.8555843229690945, 0.5663497214174213, 0.1887832404724738, 0.1887832404724738, 0.7179247593532806, 0.13648341718600343, 0.21837346749760547, 0.08189005031160206, 0.08189005031160206, 0.10918673374880274, 0.08189005031160206, 0.08189005031160206, 0.1910767840604048, 0.027296683437200684, 0.5769236193435954, 0.6148423198074616, 0.576920318253175, 0.11473490525898382, 0.27249539999008654, 0.17210235788847572, 0.07170931578686489, 0.08605117894423786, 0.10039304210161083, 0.08605117894423786, 0.08605117894423786, 0.04302558947211893, 0.18604370378212448, 0.1958354776653942, 0.10770951271596681, 0.08812596494942739, 0.15666838213231535, 0.06854241718288796, 0.04895886941634855, 0.05875064329961826, 0.08812596494942739, 0.2690052512149965, 0.12227511418863478, 0.20786769412067913, 0.03668253425659043, 0.048910045675453914, 0.13450262560749826, 0.07336506851318086, 0.012227511418863479, 0.09782009135090783, 0.14190894351796543, 0.35477235879491353, 0.10643170763847407, 0.07095447175898272, 0.07095447175898272, 0.03547723587949136, 0.07095447175898272, 0.07095447175898272, 0.10643170763847407, 0.7180623044330556, 0.1883816936752996, 0.20721986304282958, 0.1318671855727097, 0.05651450810258988, 0.16954352430776964, 0.05651450810258988, 0.07535267747011984, 0.03767633873505992, 0.0941908468376498, 0.4154559199552264, 0.1437917965601863, 0.1437917965601863, 0.1437917965601863, 0.2875835931203726, 0.1437917965601863, 0.43137538968055894, 0.17015720231831297, 0.19284482929408803, 0.12478194836676285, 0.2155324562698631, 0.05671906743943766, 0.04537525395155013, 0.10209432139098779, 0.05671906743943766, 0.03403144046366259, 0.22889379925009942, 0.22889379925009942, 0.45778759850019884, 0.16820216469590654, 0.16820216469590654, 0.1385194297495701, 0.1780964096780187, 0.06925971487478505, 0.0989424498211215, 0.1187309397853458, 0.0197884899642243, 0.0395769799284486, 0.08151908722501275, 0.24455726167503825, 0.21285539442086662, 0.14945165991252338, 0.040759543612506374, 0.072461410866678, 0.108692116300017, 0.040759543612506374, 0.05887489632917588, 0.5761770554710433, 0.5075873505801797, 0.05764691272331563, 0.11529382544663126, 0.11529382544663126, 0.3458814763398938, 0.05764691272331563, 0.11529382544663126, 0.11529382544663126, 0.11529382544663126, 0.4052040522391875, 0.6148477512646977, 0.5075512831406719, 0.6042108600169565, 0.15040087756714787, 0.13270665667689518, 0.08847110445126345, 0.13270665667689518, 0.15924798801227422, 0.08847110445126345, 0.13270665667689518, 0.07077688356101076, 0.044235552225631726, 0.205358626526192, 0.13690575101746133, 0.17113218877182665, 0.102679313263096, 0.06845287550873067, 0.27381150203492266, 0.03422643775436533, 0.03422643775436533, 0.22412333386369565, 0.0630346876491644, 0.1330732294815693, 0.14708093784805026, 0.14007708366480978, 0.042023125099442935, 0.1821002087642527, 0.035019270916202445, 0.028015416732961956, 0.2528465551603857, 0.03612093645148367, 0.14448374580593468, 0.10836280935445101, 0.07224187290296734, 0.07224187290296734, 0.28896749161186935, 0.03612093645148367, 0.27429360839123856, 0.07480734774306506, 0.17455047806715182, 0.14961469548613013, 0.04987156516204337, 0.024935782581021686, 0.22442204322919518, 0.024935782581021686, 0.024935782581021686, 0.18094028366381812, 0.11514381687697517, 0.09869470018026444, 0.06579646678684295, 0.11514381687697517, 0.11514381687697517, 0.19738940036052888, 0.032898233393421475, 0.04934735009013222, 0.6148311713464081, 0.5074873046823365, 0.6148625683996776, 0.16143749218803646, 0.16143749218803646, 0.4843124765641094, 0.11001939615567678, 0.1980349130802182, 0.26404655077362427, 0.06601163769340607, 0.08801551692454143, 0.06601163769340607, 0.06601163769340607, 0.044007758462270716, 0.06601163769340607, 0.3614356618845069, 0.3614356618845069, 0.3517589593577534, 0.3517589593577534, 0.6004130983851844, 0.7179932079421589, 0.6042234980756082, 0.5075914015223585, 0.624461724154802, 0.5769124154587856, 0.5769857327686605, 0.12624833670823898, 0.12624833670823898, 0.06312416835411949, 0.12624833670823898, 0.31562084177059746, 0.06312416835411949, 0.12624833670823898, 0.06312416835411949, 0.46011200877381336, 0.23005600438690668, 0.604154096519898, 0.6580425584040309, 0.11505657600282117, 0.051136256001253856, 0.038352192000940394, 0.29403347200720964, 0.08948844800219424, 0.025568128000626928, 0.29403347200720964, 0.038352192000940394, 0.038352192000940394, 0.44066187623541575, 0.08172122291406418, 0.14301214009961233, 0.18387275155664443, 0.1327969872353543, 0.1327969872353543, 0.14301214009961233, 0.06129091718554814, 0.06129091718554814, 0.051075764321290115, 0.576194739271481, 0.1919140365046852, 0.3838280730093704, 0.0959570182523426, 0.0959570182523426, 0.0959570182523426, 0.0959570182523426, 0.44686103527277377, 0.11171525881819344, 0.11171525881819344, 0.11171525881819344, 0.11171525881819344, 0.6581150078938228, 0.6042721789878133, 0.5209682738607474, 0.2604841369303737, 0.7192046149355373, 0.3624995080385197, 0.3624995080385197, 0.5749329988661037, 0.5075346719194164, 0.5761853907481103, 0.5761993204344452, 0.6040695938309938, 0.45296006675520495, 0.576148542451432, 0.16125127656625263, 0.19708489358097542, 0.10750085104416841, 0.10750085104416841, 0.089584042536807, 0.10750085104416841, 0.10750085104416841, 0.089584042536807, 0.035833617014722804, 0.7180096764749012, 0.6148811793748898, 0.5761756291355731, 0.45280150722223805, 0.6523932585189696, 0.21746441950632317, 0.35075298188713405, 0.35075298188713405, 0.3869489442211314, 0.1934744721105657, 0.1934744721105657, 0.1934744721105657, 0.7180264979518901, 0.2837710432828971, 0.09459034776096571, 0.0630602318406438, 0.2522409273625752, 0.0315301159203219, 0.0630602318406438, 0.1261204636812876, 0.0315301159203219, 0.0315301159203219, 0.07302727608451433, 0.07302727608451433, 0.21908182825354297, 0.3651363804225716, 0.07302727608451433, 0.07302727608451433, 0.14605455216902866, 0.07302727608451433, 0.4523784093731432, 0.0847034682252741, 0.0847034682252741, 0.42351734112637046, 0.1694069364505482, 0.0847034682252741, 0.5417808305005751, 0.16422720748594163, 0.17009246489615384, 0.14076617784509285, 0.10557463338381963, 0.13490092043488064, 0.08211360374297082, 0.07038308892254642, 0.07038308892254642, 0.052787316691909815, 0.18993651765296876, 0.12662434510197917, 0.06331217255098959, 0.06331217255098959, 0.12662434510197917, 0.09496825882648438, 0.09496825882648438, 0.12662434510197917, 0.12662434510197917, 0.5904544920086885, 0.13602965232886144, 0.2511316658378981, 0.15695729114868628, 0.09417437468921178, 0.07324673586938693, 0.11510201350903661, 0.06278291645947452, 0.09417437468921178, 0.03139145822973726, 0.5417322215031226, 0.5762074284493645, 0.3374555961214207, 0.048207942303060095, 0.2410397115153005, 0.048207942303060095, 0.048207942303060095, 0.09641588460612019, 0.048207942303060095, 0.19283176921224038, 0.718061958551099, 0.7603498469682158, 0.8312027205414427, 0.6042021535925586, 0.216231430650502, 0.15779050344766363, 0.0935054835245414, 0.09934957624482524, 0.08181729808397373, 0.07012911264340606, 0.16947868888823128, 0.07597320536368989, 0.040908649041986864, 0.15209710692666686, 0.16730681761933353, 0.24335537108266697, 0.045629132078000054, 0.09125826415600011, 0.09125826415600011, 0.07604855346333343, 0.07604855346333343, 0.045629132078000054, 0.347324510960403, 0.13892980438416122, 0.13892980438416122, 0.09261986958944081, 0.023154967397360202, 0.046309934794720405, 0.09261986958944081, 0.023154967397360202, 0.046309934794720405, 0.6580283658222982, 0.1729357945265273, 0.08646789726326365, 0.043233948631631824, 0.08646789726326365, 0.1729357945265273, 0.25940369178979095, 0.08646789726326365, 0.08646789726326365, 0.2628427423932377, 0.18774481599516976, 0.07509792639806791, 0.07509792639806791, 0.07509792639806791, 0.07509792639806791, 0.07509792639806791, 0.15019585279613581, 0.037548963199033954, 0.7180028789811406, 0.5975830676831762, 0.576221393343376, 0.5749442404166103, 0.5417156282133957, 0.40244467067955614, 0.5872918352393289, 0.29364591761966446, 0.6042164410885902, 0.19163297375897245, 0.11497978425538347, 0.24273510009469845, 0.03832659475179449, 0.07665318950358899, 0.07665318950358899, 0.08942872108752048, 0.08942872108752048, 0.06387765791965748, 0.5417499660514039, 0.5761802103428981, 0.1862071323350576, 0.0931035661675288, 0.4655178308376439, 0.1862071323350576, 0.09865921061075991, 0.13565641458979488, 0.16032121724248485, 0.11099161193710491, 0.11099161193710491, 0.13565641458979488, 0.08632680928441493, 0.1233240132634499, 0.03699720397903497, 0.19068538115232184, 0.15254830492185747, 0.10169886994790497, 0.10169886994790497, 0.06356179371744061, 0.08898651120441685, 0.20339773989580995, 0.03813707623046437, 0.06356179371744061, 0.5769206788476666, 0.5417800738031358, 0.10381946463675029, 0.15572919695512544, 0.11420141110042532, 0.08305557170940024, 0.08305557170940024, 0.22840282220085065, 0.11420141110042532, 0.07267362524572521, 0.051909732318375146, 0.12787661095733424, 0.11874256731752465, 0.155278741876763, 0.17354682915638217, 0.10960852367771506, 0.0822063927582863, 0.09134043639809589, 0.0822063927582863, 0.05480426183885753, 0.4451092816894633, 0.5761154980454548, 0.09477575805945045, 0.1658575766040383, 0.09477575805945045, 0.07108181854458784, 0.26063333466348876, 0.09477575805945045, 0.07108181854458784, 0.14216363708917568, 0.02369393951486261, 0.7180049408068958, 0.6386737904905055, 0.21289126349683515, 0.5749048164842454, 0.5760402574951069, 0.13333477354858625, 0.20000216032287937, 0.10000108016143969, 0.10000108016143969, 0.1666684669357328, 0.06666738677429312, 0.06666738677429312, 0.1666684669357328, 0.03333369338714656, 0.614839732969007, 0.4011232452167349, 0.5769060375132713, 0.5769478199671226, 0.5768919711460725, 0.7179635267277416, 0.19164499517375383, 0.1836597870415141, 0.1397411423141955, 0.155711558578675, 0.06787426912403781, 0.09182989352075704, 0.07186687319015768, 0.05988906099179807, 0.03992604066119872, 0.576907454274218, 0.5286175775969633, 0.26168629983240027, 0.13755305504010784, 0.0939386717347078, 0.12413324479229244, 0.08387381404884624, 0.07716390892493855, 0.11742333966838474, 0.07380895636298469, 0.033549525619538494, 0.15509439420969626, 0.15509439420969626, 0.1061172170908448, 0.16325725706283817, 0.12244294279712863, 0.1061172170908448, 0.0979543542377029, 0.04897717711885145, 0.04897717711885145, 0.6148481995453009, 0.13989497724502437, 0.10948302567001907, 0.15205975787502649, 0.21896605134003813, 0.18855409976503285, 0.0729886837800127, 0.05474151283500953, 0.04561792736250794, 0.018247170945003176, 0.5417628890653124, 0.7180359412937538, 0.5417455510669813, 0.7063567029163031, 0.42947929183751876, 0.6148697239865988, 0.19896557131042766, 0.3481897497932484, 0.09948278565521383, 0.09948278565521383, 0.09948278565521383, 0.049741392827606916, 0.049741392827606916, 0.049741392827606916, 0.5761665785478002, 0.10307271998478774, 0.13743029331305032, 0.08589393332065645, 0.2233242266337068, 0.05153635999239387, 0.13743029331305032, 0.06871514665652516, 0.10307271998478774, 0.08589393332065645, 0.3833557643397723, 0.17038033970656546, 0.042595084926641366, 0.12778525477992408, 0.042595084926641366, 0.12778525477992408, 0.042595084926641366, 0.042595084926641366, 0.7044150318866196, 0.16773916865825586, 0.36196346920992056, 0.09711215027583234, 0.04708467892161568, 0.07062701838242352, 0.07651260324762547, 0.0588558486520196, 0.08534098054542842, 0.03531350919121176, 0.10533525203024836, 0.2984498807523704, 0.1228911273686231, 0.0702235013534989, 0.08777937669187363, 0.08777937669187363, 0.03511175067674945, 0.1228911273686231, 0.05266762601512418, 0.57489783410411, 0.6581126539038833, 0.2309701392973718, 0.2309701392973718, 0.2309701392973718, 0.7625652548570634, 0.5075493781405465, 0.5769643072177955, 0.06323824147215729, 0.06323824147215729, 0.06323824147215729, 0.06323824147215729, 0.442667690305101, 0.06323824147215729, 0.12647648294431457, 0.06323824147215729, 0.19494809215134679, 0.029992014177130277, 0.13496406379708625, 0.1499600708856514, 0.05998402835426055, 0.22494010632847708, 0.16495607797421652, 0.014996007088565138, 0.029992014177130277, 0.11750200643899614, 0.11750200643899614, 0.05875100321949807, 0.05875100321949807, 0.05875100321949807, 0.29375501609749033, 0.23500401287799227, 0.05875100321949807, 0.21310209043857517, 0.13061095865590092, 0.1512337416015695, 0.041245565891337134, 0.10311391472834283, 0.09623965374645331, 0.11686243669212187, 0.1237366976740114, 0.027497043927558088, 0.26162051528960445, 0.26162051528960445, 0.26162051528960445, 0.5761242846823685, 0.38328240569805927, 0.710425000173661, 0.4140097900667209, 0.09124212083532078, 0.09124212083532078, 0.09124212083532078, 0.3649684833412831, 0.09124212083532078, 0.09124212083532078, 0.09124212083532078, 0.18248424167064156, 0.39250605135060956, 0.15788978721056685, 0.15788978721056685, 0.07894489360528342, 0.15788978721056685, 0.39472446802641714, 0.5749472202739939, 0.5748438363051438, 0.11481152258403875, 0.5740576129201937, 0.11481152258403875, 0.11481152258403875, 0.6580446764816117, 0.14263973596208465, 0.16301684109952533, 0.14263973596208465, 0.16301684109952533, 0.04075421027488133, 0.14263973596208465, 0.04075421027488133, 0.16301684109952533, 0.5074849485028167, 0.5076041018275579, 0.5074962010132105, 0.1674699450426601, 0.11722896152986208, 0.08373497252133005, 0.1507229505383941, 0.10048196702559607, 0.1842169395469261, 0.08373497252133005, 0.08373497252133005, 0.033493989008532024, 0.5417533300745534, 0.6148883639164232, 0.10809203721582164, 0.17294725954531462, 0.1441227162877622, 0.10809203721582164, 0.12971044465898596, 0.08647362977265731, 0.10809203721582164, 0.10088590140143353, 0.03603067907194055, 0.07185116498163954, 0.07185116498163954, 0.21555349494491866, 0.21555349494491866, 0.35925582490819774, 0.07185116498163954, 0.703072813109092, 0.5417657383095598, 0.5417355688594813, 0.5749239124132893, 0.22010510005892645, 0.13206306003535587, 0.08804204002357058, 0.14673673337261764, 0.17608408004714116, 0.08804204002357058, 0.058694693349047054, 0.04402102001178529, 0.058694693349047054, 0.10689187335303584, 0.09866942155664847, 0.032889807185549486, 0.2384511020952338, 0.1480041323349727, 0.1480041323349727, 0.05755716257471161, 0.13978168053858533, 0.008222451796387371, 0.1009514939547297, 0.12618936744341214, 0.07571362046604728, 0.2019029879094594, 0.1009514939547297, 0.12618936744341214, 0.05047574697736485, 0.17666511442077698, 0.025237873488682425, 0.576150549721315, 0.7028405600718404, 0.16587293572926196, 0.07372130476856087, 0.22116391430568258, 0.12901228334498152, 0.07372130476856087, 0.12901228334498152, 0.07372130476856087, 0.055290978576420645, 0.09215163096070107, 0.6580967129371841, 0.5748507462342054, 0.6580231622601044, 0.31971821335701733, 0.31971821335701733, 0.6581854842695787, 0.2235247637821428, 0.1117623818910714, 0.26822971653857136, 0.022352476378214282, 0.1117623818910714, 0.06705742913464284, 0.1117623818910714, 0.044704952756428565, 0.044704952756428565, 0.7466381483701919, 0.8312458296833096, 0.1648424883567586, 0.1585023926507294, 0.12363186626756895, 0.10778162700249601, 0.08559129203139389, 0.10144153129646682, 0.14899224909168565, 0.06023090920727718, 0.04755071779521883, 0.29990782247747455, 0.29990782247747455, 0.1430752394764674, 0.15813579100030606, 0.13554496371454805, 0.1656660667622254, 0.13554496371454805, 0.0828330333811127, 0.0828330333811127, 0.07530275761919336, 0.02259082728575801, 0.40445969942946497, 0.06740994990491082, 0.06740994990491082, 0.06740994990491082, 0.06740994990491082, 0.06740994990491082, 0.13481989980982165, 0.06740994990491082, 0.08321633728737207, 0.11888048183910295, 0.09510438547128236, 0.19020877094256472, 0.11888048183910295, 0.10699243365519266, 0.13076853002301325, 0.07132828910346177, 0.08321633728737207, 0.6148480336961083, 0.6579976879526691, 0.6677606940480129, 0.2225868980160043, 0.1521181802419187, 0.24892065857768514, 0.19360495667153288, 0.13828925476538062, 0.027657850953076126, 0.08297355285922837, 0.08297355285922837, 0.05531570190615225, 0.027657850953076126, 0.1486178493535256, 0.05944713974141024, 0.10403249454746792, 0.10403249454746792, 0.0743089246767628, 0.11889427948282048, 0.23778855896564097, 0.08917070961211536, 0.0743089246767628, 0.07698636019343759, 0.46191816116062556, 0.07698636019343759, 0.07698636019343759, 0.07698636019343759, 0.15397272038687518, 0.5748568227213182, 0.2090490106706584, 0.11067300564917208, 0.1967520100429726, 0.0860790043938005, 0.11067300564917208, 0.06148500313842894, 0.0983760050214863, 0.04918800251074315, 0.06148500313842894, 0.11704763098427794, 0.11704763098427794, 0.23409526196855587, 0.11704763098427794, 0.3511428929528338, 0.16812207320447387, 0.5043662196134215, 0.16812207320447387, 0.11531132069872592, 0.16656079656482634, 0.12812368966525103, 0.10249895173220082, 0.10249895173220082, 0.10249895173220082, 0.12812368966525103, 0.06406184483262552, 0.07687421379915062, 0.11328946058615359, 0.2517543568581191, 0.15105261411487148, 0.06293858921452977, 0.11328946058615359, 0.12587717842905954, 0.07552630705743574, 0.05035087137162382, 0.05035087137162382, 0.363100674133409, 0.363100674133409, 0.5074863884924365, 0.21608506357535356, 0.21608506357535356, 0.43217012715070713, 0.6042359052390669, 0.47580885940336565, 0.23790442970168282, 0.24383958857446472, 0.24383958857446472, 0.48767917714892944, 0.145936802833266, 0.145936802833266, 0.145936802833266, 0.145936802833266, 0.145936802833266, 0.291873605666532, 0.5417411992155434, 0.1588206374545479, 0.35734643427273277, 0.11911547809091093, 0.03970515936363698, 0.07941031872727396, 0.07941031872727396, 0.03970515936363698, 0.1588206374545479, 0.03970515936363698, 0.11224151400708647, 0.14030189250885808, 0.14030189250885808, 0.14030189250885808, 0.08418113550531485, 0.22448302801417294, 0.08418113550531485, 0.08418113550531485, 0.056120757003543235, 0.41264952468728966, 0.5761542627590012, 0.3386433965594436, 0.3386433965594436, 0.5749277415585738, 0.6580952404152848, 0.06271068251708384, 0.18813204755125154, 0.3762640951025031, 0.06271068251708384, 0.12542136503416768, 0.06271068251708384, 0.06271068251708384, 0.06271068251708384, 0.7179529050343568, 0.17258423272893586, 0.2416179258205102, 0.06903369309157434, 0.06903369309157434, 0.06903369309157434, 0.06903369309157434, 0.27613477236629735, 0.06903369309157434, 0.03451684654578717, 0.1856767247440354, 0.13681442875876293, 0.14658688795581742, 0.07817967357643596, 0.15635934715287192, 0.048862295985272475, 0.09772459197054495, 0.08795213277349045, 0.048862295985272475, 0.5761619697334969, 0.3178464432400744, 0.0794616108100186, 0.0794616108100186, 0.0794616108100186, 0.0794616108100186, 0.0794616108100186, 0.23838483243005582, 0.33862848570312876, 0.12190625485312635, 0.0812708365687509, 0.07449826685468833, 0.08804340628281347, 0.09481597599687605, 0.09481597599687605, 0.0812708365687509, 0.020317709142187727, 0.5761892631967807, 0.12977945554310977, 0.4758580036580692, 0.08651963702873985, 0.04325981851436993, 0.04325981851436993, 0.12977945554310977, 0.04325981851436993, 0.08651963702873985, 0.04325981851436993, 0.14702325055716498, 0.12251937546430415, 0.14702325055716498, 0.07351162527858249, 0.12251937546430415, 0.09801550037144331, 0.14702325055716498, 0.049007750185721656, 0.09801550037144331, 0.4559207278685403, 0.23406251600523703, 0.09362500640209481, 0.1560416773368247, 0.14823959346998347, 0.06241667093472988, 0.10142709026893605, 0.09362500640209481, 0.07021875480157111, 0.046812503201047406, 0.11213489117274639, 0.11213489117274639, 0.07475659411516426, 0.280337227931866, 0.16820233675911958, 0.07475659411516426, 0.09344574264395532, 0.056067445586373194, 0.018689148528791066, 0.20342058402512572, 0.20342058402512572, 0.6102617520753771, 0.5417434063367775, 0.6580552523728891, 0.12790100109038108, 0.1438886262266787, 0.15987625136297634, 0.22382675190816687, 0.06395050054519054, 0.0959257508177858, 0.12790100109038108, 0.0479628754088929, 0.0479628754088929, 0.3841993245881774, 0.18009343340070816, 0.07203737336028326, 0.08404360225366381, 0.06003114446690272, 0.048024915573522176, 0.048024915573522176, 0.08404360225366381, 0.03601868668014163, 0.3860326947791842, 0.11455927214301251, 0.23866515029794272, 0.17183890821451878, 0.09546606011917709, 0.1050126661310948, 0.09546606011917709, 0.057279636071506254, 0.09546606011917709, 0.028639818035753127, 0.1856442460432064, 0.1485153968345651, 0.12995097223024446, 0.0928221230216032, 0.11138654762592383, 0.055693273812961915, 0.07425769841728255, 0.1485153968345651, 0.055693273812961915, 0.4135899327304678, 0.7180349856851933, 0.13537907523221657, 0.10153430642416243, 0.13537907523221657, 0.3046029192724873, 0.03384476880805414, 0.03384476880805414, 0.20306861284832486, 0.03384476880805414, 0.03384476880805414, 0.5750782739533531, 0.2081094500320998, 0.3745970100577796, 0.08324378001283991, 0.041621890006419957, 0.08324378001283991, 0.041621890006419957, 0.08324378001283991, 0.041621890006419957, 0.041621890006419957, 0.5075648398741002, 0.5748575266585665, 0.604235189999997, 0.5748739303092736, 0.16075709033549296, 0.15376765162525416, 0.15376765162525416, 0.12580989678429885, 0.10018195484675649, 0.14211858710818945, 0.07688382581262708, 0.04426644516484589, 0.039606819358020004, 0.12867096688952032, 0.09650322516714023, 0.09650322516714023, 0.09650322516714023, 0.09650322516714023, 0.1608387086119004, 0.09650322516714023, 0.06433548344476016, 0.1608387086119004, 0.5749006789956683, 0.23191260839249966, 0.46382521678499933, 0.07922520090878218, 0.35651340408951987, 0.15845040181756437, 0.07922520090878218, 0.07922520090878218, 0.11883780136317328, 0.11883780136317328, 0.7180648979565746, 0.6041263246021678, 0.38550128846720555, 0.14456298317520208, 0.09637532211680139, 0.14456298317520208, 0.048187661058400694, 0.048187661058400694, 0.048187661058400694, 0.048187661058400694, 0.048187661058400694, 0.12609605798578566, 0.2143632985758356, 0.10087684638862851, 0.08826724059004995, 0.11348645218720708, 0.18914408697867846, 0.050438423194314255, 0.06304802899289283, 0.037828817395735695, 0.5075032737549198, 0.1256666281110075, 0.1256666281110075, 0.1256666281110075, 0.251333256222015, 0.17593327935541053, 0.05026665124440301, 0.10053330248880601, 0.05026665124440301, 0.025133325622201504, 0.5748610176483359, 0.17566427188792613, 0.24153837384589844, 0.1317482039159446, 0.24153837384589844, 0.021958033985990767, 0.04391606797198153, 0.08783213594396307, 0.04391606797198153, 0.04391606797198153, 0.2710067590836359, 0.2710067590836359, 0.2710067590836359, 0.2935834230565178, 0.1691651694191831, 0.1287838063965394, 0.07421439690648034, 0.10804743079031696, 0.05456940949005907, 0.07857994966568506, 0.04583830397164962, 0.0469296921614508, 0.6581420491650268, 0.2472240060259408, 0.1498327309248126, 0.08989963855488756, 0.10488291164736882, 0.11986618473985008, 0.0749163654624063, 0.10488291164736882, 0.05244145582368441, 0.05244145582368441, 0.6148842029840338, 0.11957247237491479, 0.11957247237491479, 0.17935870856237218, 0.11957247237491479, 0.059786236187457396, 0.298931180937287, 0.059786236187457396, 0.059786236187457396, 0.059786236187457396, 0.45189824644741716, 0.22495705555336057, 0.3655552152742109, 0.11247852777668028, 0.11247852777668028, 0.02811963194417007, 0.02811963194417007, 0.02811963194417007, 0.08435889583251022, 0.02811963194417007, 0.09600580817678836, 0.3264197478010804, 0.11520696981214602, 0.07680464654143068, 0.03840232327071534, 0.1344081314475037, 0.11520696981214602, 0.07680464654143068, 0.05760348490607301, 0.04191780680309552, 0.12575342040928655, 0.1676712272123821, 0.37726026122785966, 0.04191780680309552, 0.08383561360619105, 0.08383561360619105, 0.04191780680309552, 0.08383561360619105, 0.20624999781748174, 0.08839285620749217, 0.14732142701248696, 0.13258928431123826, 0.058928570804994786, 0.10312499890874087, 0.22098214051873044, 0.029464285402497393, 0.029464285402497393, 0.382933504156047, 0.08836927018985699, 0.058912846793237994, 0.17673854037971398, 0.029456423396618997, 0.11782569358647599, 0.029456423396618997, 0.08836927018985699, 0.029456423396618997, 0.5769588750698214, 0.6042003848295304, 0.5417285747297296, 0.14519919812289184, 0.07259959906144592, 0.1814989976536148, 0.07259959906144592, 0.03629979953072296, 0.2903983962457837, 0.10889939859216888, 0.03629979953072296, 0.03629979953072296, 0.17203764972397434, 0.11131847923315986, 0.13155820273009802, 0.08095889398775262, 0.08095889398775262, 0.15179792622703617, 0.19227737322091248, 0.06071917049081447, 0.030359585245407234, 0.5769670663291494, 0.6580616026237825, 0.7180349286055367, 0.23099997274276626, 0.061147051608379306, 0.0951176358352567, 0.14267645375288504, 0.23779408958814174, 0.061147051608379306, 0.0951176358352567, 0.05435293476300383, 0.027176467381501916, 0.5761217380759327, 0.5075089833442451, 0.7630016208170317, 0.17653224652107352, 0.17653224652107352, 0.08826612326053676, 0.07723285785296967, 0.12136591948323805, 0.15446571570593934, 0.09929938866810385, 0.06619959244540256, 0.03309979622270128, 0.16285888921614938, 0.19246959634635835, 0.10363747495573142, 0.1332481820859404, 0.07402676782552245, 0.16285888921614938, 0.04441606069531347, 0.07402676782552245, 0.05922141426041796, 0.7181037456973426, 0.6148691029664253, 0.17442093325081842, 0.07267538885450768, 0.06540784996905691, 0.22529370544897379, 0.14535077770901536, 0.05087277219815537, 0.07994292773995844, 0.11628062216721229, 0.07267538885450768, 0.5748678450663914, 0.5075346169278224, 0.28875325570001065, 0.28875325570001065, 0.05351131927283453, 0.05351131927283453, 0.32106791563700715, 0.10702263854566905, 0.32106791563700715, 0.10702263854566905, 0.05351131927283453, 0.1160855659900701, 0.2321711319801402, 0.1160855659900701, 0.2321711319801402, 0.1160855659900701, 0.2321711319801402, 0.3240867352453495, 0.10802891174844982, 0.021605782349689965, 0.10802891174844982, 0.08642312939875986, 0.17284625879751972, 0.08642312939875986, 0.08642312939875986, 0.021605782349689965, 0.11967881661532088, 0.17096973802188697, 0.1367757904175096, 0.10258184281313218, 0.08548486901094349, 0.18806671182407567, 0.10258184281313218, 0.0341939476043774, 0.05129092140656609, 0.7179523821159491, 0.13658403220233908, 0.14204739349043263, 0.1475107547785262, 0.049170251592842065, 0.20214436765946184, 0.12019394833805838, 0.087413780609497, 0.08195041932140344, 0.03824352901665494, 0.7180040393067414, 0.1261934650875348, 0.1261934650875348, 0.1261934650875348, 0.1261934650875348, 0.37858039526260434, 0.1261934650875348, 0.5769972013418057, 0.5761442247426818, 0.5761023891606235, 0.541784675056787, 0.10743960525954509, 0.14325280701272677, 0.14325280701272677, 0.10743960525954509, 0.32231881577863525, 0.07162640350636339, 0.03581320175318169, 0.07162640350636339, 0.03581320175318169, 0.5075241319961232, 0.14368001294706517, 0.17960001618383145, 0.1364960122997119, 0.11494401035765213, 0.07902400712088584, 0.0862080077682391, 0.1221280110050054, 0.09339200841559235, 0.04310400388411955, 0.5748245370785214, 0.3174280325851624, 0.14813308187307578, 0.06348560651703247, 0.0846474753560433, 0.10580934419505413, 0.06348560651703247, 0.10580934419505413, 0.04232373767802165, 0.04232373767802165, 0.6148810939819627, 0.6041394322993626, 0.8310420246480211, 0.5075975879944269, 0.541718073816133, 0.5761746328699385, 0.5912957571306726, 0.11825915142613452, 0.11825915142613452, 0.05958075017917526, 0.3872748761646392, 0.11916150035835052, 0.14895187544793814, 0.05958075017917526, 0.05958075017917526, 0.11916150035835052, 0.05958075017917526, 0.02979037508958763, 0.5748528294156583, 0.6581227835667843, 0.5865307892128118, 0.2932653946064059, 0.16195988848677031, 0.16195988848677031, 0.4858796654603109, 0.5749012101411518, 0.12055537954405697, 0.18083306931608545, 0.18083306931608545, 0.060277689772028485, 0.060277689772028485, 0.060277689772028485, 0.060277689772028485, 0.060277689772028485, 0.24111075908811394, 0.14786312102338858, 0.14786312102338858, 0.14786312102338858, 0.14786312102338858, 0.4435893630701658, 0.7858470737827831, 0.5761749055623067, 0.14681115335435363, 0.08563983945670628, 0.22021673003153042, 0.08563983945670628, 0.08563983945670628, 0.13457689057482414, 0.06117131389764734, 0.12234262779529469, 0.04893705111811787, 0.13725319015050993, 0.2058797852257649, 0.13725319015050993, 0.06862659507525497, 0.3431329753762748, 0.06862659507525497, 0.09183149736886441, 0.09183149736886441, 0.09183149736886441, 0.36732598947545764, 0.09183149736886441, 0.09183149736886441, 0.09183149736886441, 0.6041505139672314, 0.6464191918884218, 0.20812123087097856, 0.4162424617419571, 0.11994917854465073, 0.11994917854465073, 0.11994917854465073, 0.09995764878720893, 0.21990682733185968, 0.09995764878720893, 0.059974589272325365, 0.11994917854465073, 0.059974589272325365, 0.14930479586219164, 0.23888767337950662, 0.059721918344876654, 0.10451335710353414, 0.07465239793109582, 0.08958287751731497, 0.1642352754484108, 0.04479143875865749, 0.059721918344876654, 0.7881537433971341, 0.09406683278487828, 0.09406683278487828, 0.09406683278487828, 0.09406683278487828, 0.18813366556975655, 0.09406683278487828, 0.3762673311395131, 0.09406683278487828, 0.288352139806143, 0.10813205242730363, 0.10813205242730363, 0.07208803495153575, 0.07208803495153575, 0.10813205242730363, 0.1441760699030715, 0.07208803495153575, 0.018022008737883937, 0.1209427854509388, 0.2418855709018776, 0.0806285236339592, 0.18813322181257147, 0.1209427854509388, 0.09406661090628574, 0.06719043636163267, 0.0403142618169796, 0.05375234908930614, 0.6041387051475421, 0.7620496048745825, 0.5076015093117716, 0.19237784965571542, 0.19237784965571542, 0.38475569931143083, 0.576143272648764, 0.6581151521659583, 0.13293235860707697, 0.26586471721415394, 0.13293235860707697, 0.13293235860707697, 0.03323308965176924, 0.06646617930353849, 0.11631581378119234, 0.08308272412942311, 0.049849634477653865, 0.33481569261447564, 0.08810939279328306, 0.17621878558656612, 0.05286563567596984, 0.07048751423462644, 0.12335314991059629, 0.07048751423462644, 0.05286563567596984, 0.03524375711731322, 0.3411906323390769, 0.3411906323390769, 0.6148557515784993, 0.5748951170350977, 0.7169883317655261, 0.6041693698012836, 0.22446053717677575, 0.22446053717677575, 0.4489210743535515, 0.9052928006272579, 0.5760888502598064, 0.45290162546618634, 0.6580859435870214, 0.5074317313233286, 0.5075782509303387, 0.6148065701180213, 0.5761615574377998, 0.11425377769383875, 0.25027017971031346, 0.15777902633911065, 0.0435252486452719, 0.12513508985515673, 0.10881312161317976, 0.08160984120988482, 0.0870504972905438, 0.032643936483953924, 0.1794332338211408, 0.1794332338211408, 0.1794332338211408, 0.3588664676422816, 0.5075404550231527, 0.576922179003616, 0.34109980799804135, 0.34109980799804135, 0.4015094414440072, 0.5749469464662894, 0.6147962961440439, 0.7180678385867559, 0.5075567003279444, 0.7180799451570788, 0.5749193227990582, 0.17944680004941327, 0.1888913684730666, 0.07555654738922664, 0.11333482108383995, 0.08500111581287996, 0.11333482108383995, 0.08500111581287996, 0.11333482108383995, 0.04722284211826665, 0.11061635677679638, 0.11061635677679638, 0.05530817838839819, 0.05530817838839819, 0.16592453516519456, 0.3871572487187873, 0.11061635677679638, 0.4463545572779001, 0.07439242621298335, 0.1487848524259667, 0.07439242621298335, 0.07439242621298335, 0.07439242621298335, 0.07439242621298335, 0.4525322276593759, 0.5075846517167597, 0.507589874804116, 0.576913107486902, 0.45268555568976504, 0.763283395770903, 0.2770461778897864, 0.2770461778897864, 0.0692615444724466, 0.2770461778897864, 0.0692615444724466, 0.0692615444724466, 0.17357566998251064, 0.27482814413897516, 0.08678783499125532, 0.08678783499125532, 0.05785855666083688, 0.1446463916520922, 0.08678783499125532, 0.04339391749562766, 0.04339391749562766, 0.2134728949642537, 0.4269457899285074, 0.07180227077258979, 0.3590113538629489, 0.07180227077258979, 0.10770340615888467, 0.03590113538629489, 0.10770340615888467, 0.10770340615888467, 0.10770340615888467, 0.03590113538629489, 0.30345091945781244, 0.6069018389156249, 0.20822875685732584, 0.20822875685732584, 0.4164575137146517, 0.5761856755383851, 0.6148881419230504, 0.7582883636300289, 0.7179042788447095, 0.5150119443384286, 0.10300238886768572, 0.10300238886768572, 0.10300238886768572, 0.13750839323337197, 0.06875419661668598, 0.3437709830834299, 0.13750839323337197, 0.06875419661668598, 0.13750839323337197, 0.06875419661668598, 0.7589640310898383, 0.6580346073606899, 0.15158808128623846, 0.12483724341219637, 0.14267113532822442, 0.10700335149616833, 0.11592029745418235, 0.09808640553815429, 0.17833891916028052, 0.05350167574808416, 0.035667783832056106, 0.5748698120404162, 0.11643890272780136, 0.19794613463726232, 0.17465835409170205, 0.15719251868253184, 0.09315112218224109, 0.08150723190946095, 0.05239750622751061, 0.05821945136390068, 0.06404139650029075, 0.7180604467819233, 0.22885671327093424, 0.22885671327093424, 0.4577134265418685, 0.5748570378166284, 0.12666930713596333, 0.19000396070394499, 0.12666930713596333, 0.06333465356798167, 0.12666930713596333, 0.12666930713596333, 0.06333465356798167, 0.06333465356798167, 0.19000396070394499, 0.39195538585335227, 0.06532589764222538, 0.06532589764222538, 0.13065179528445076, 0.06532589764222538, 0.13065179528445076, 0.06532589764222538, 0.06532589764222538, 0.20637937838072082, 0.10318968919036041, 0.10318968919036041, 0.10318968919036041, 0.10318968919036041, 0.10318968919036041, 0.10318968919036041, 0.3095690675710812, 0.5824821476676457, 0.1365333126970545, 0.21845330031528717, 0.10922665015764359, 0.24575996285469806, 0.08191998761823269, 0.08191998761823269, 0.054613325078821794, 0.054613325078821794, 0.054613325078821794, 0.1290344873106633, 0.3871034619319899, 0.1290344873106633, 0.1290344873106633, 0.1290344873106633, 0.5417021057424826, 0.7179220777763735, 0.6148643010027803, 0.10874113946366708, 0.4349645578546683, 0.10874113946366708, 0.21748227892733416, 0.10874113946366708, 0.6054072293049728, 0.2248433574609438, 0.4496867149218876, 0.2248433574609438, 0.6041881579484643, 0.47140455468167114, 0.09428091093633423, 0.18856182187266846, 0.09428091093633423, 0.09428091093633423, 0.09428091093633423, 0.0860999118558843, 0.27551971793882973, 0.10331989422706116, 0.06887992948470743, 0.06887992948470743, 0.18941980608294545, 0.10331989422706116, 0.05165994711353058, 0.03443996474235372, 0.39671146175733124, 0.26235558285102817, 0.26235558285102817, 0.26235558285102817, 0.10308595879516062, 0.19758142102405785, 0.15462893819274093, 0.15462893819274093, 0.09449546222889724, 0.11167645536142401, 0.07731446909637046, 0.06872397253010708, 0.03436198626505354, 0.7633041605455362, 0.11892608860919061, 0.2735300038011384, 0.08324826202643343, 0.09514087088735249, 0.04757043544367624, 0.05946304430459531, 0.23785217721838123, 0.05946304430459531, 0.02378521772183812, 0.5417333340598715, 0.5075195817458801, 0.6041553394103273, 0.5748727499597364, 0.5109691101343263, 0.25548455506716317, 0.2046456489913445, 0.6139369469740334, 0.20603132338728222, 0.025753915423410278, 0.20603132338728222, 0.16310813101493177, 0.06009246932129065, 0.08584638474470092, 0.18027740796387196, 0.025753915423410278, 0.04292319237235046, 0.7180671667072045, 0.157679612550913, 0.1734475738060043, 0.1103757287856391, 0.13875805904480346, 0.10722213653462084, 0.11352932103665737, 0.09145417527952955, 0.05045747601629216, 0.059918252769346945, 0.11123764403915937, 0.2502846990881086, 0.15295176055384413, 0.11123764403915937, 0.08342823302936953, 0.11818999679160683, 0.0695235275244746, 0.06257117477202714, 0.04866646926713222, 0.5417982913479344, 0.0848028812038781, 0.1611254742873684, 0.2289677792504709, 0.11872403368542934, 0.11024374556504155, 0.06784230496310249, 0.09328316932426592, 0.0848028812038781, 0.050881728722326866, 0.5994812429139179, 0.09960926283543532, 0.14388004631785103, 0.1770831339296628, 0.05533847935301962, 0.18815082980026673, 0.12174465457664317, 0.05533847935301962, 0.11067695870603925, 0.0442707834824157, 0.5748403971264788, 0.6041577813681179, 0.5917037597330924, 0.2958518798665462, 0.6147935638579618, 0.4477334640383906, 0.08954669280767812, 0.08954669280767812, 0.17909338561535623, 0.08954669280767812, 0.4351625239150532, 0.08703250478301064, 0.17406500956602128, 0.08703250478301064, 0.08703250478301064, 0.08703250478301064, 0.5699426188104012, 0.2849713094052006, 0.19987310011749798, 0.19987310011749798, 0.39974620023499596, 0.5417511275193198, 0.09092355566165149, 0.1250198890347708, 0.1250198890347708, 0.15911622240789008, 0.1250198890347708, 0.06819266674623861, 0.07955811120394504, 0.15911622240789008, 0.056827222288532174, 0.6464358697241206, 0.19644581793896757, 0.1384050080933635, 0.12501097505207026, 0.05804080984560405, 0.1160816196912081, 0.12054629737163919, 0.1384050080933635, 0.05357613216517297, 0.05804080984560405, 0.7179893112360047, 0.5748607870950587, 0.5761534248167046, 0.6148717180186598, 0.48228701403582463, 0.2763999956193863, 0.5527999912387725, 0.1412105474807888, 0.2824210949615776, 0.1412105474807888, 0.1412105474807888, 0.2824210949615776, 0.5355439479119404, 0.2677719739559702, 0.07581138343548549, 0.1768932280161328, 0.1768932280161328, 0.10108184458064731, 0.25270461145161827, 0.050540922290323656, 0.07581138343548549, 0.07581138343548549, 0.025270461145161828, 0.2554247812237419, 0.12771239061187095, 0.08939867342830966, 0.08939867342830966, 0.14048362967305805, 0.08939867342830966, 0.07662743436712256, 0.06385619530593548, 0.07662743436712256, 0.18365420991388934, 0.18905580432312138, 0.08102391613848059, 0.1512446434584971, 0.11883507700310486, 0.11883507700310486, 0.08102391613848059, 0.04321275527385631, 0.03240956645539224, 0.20733394074216466, 0.05528905086457724, 0.11057810172915448, 0.17968941530987603, 0.09675583901301017, 0.17968941530987603, 0.09675583901301017, 0.05528905086457724, 0.01382226271614431, 0.15528357557783298, 0.0887334717587617, 0.15528357557783298, 0.04436673587938085, 0.06655010381907128, 0.13310020763814256, 0.19965031145721385, 0.11091683969845213, 0.04436673587938085, 0.6581059335498999, 0.2108569310564475, 0.14935699283165033, 0.140571287370965, 0.19328552013507688, 0.08346420187651048, 0.07907134914616781, 0.08346420187651048, 0.043928527303426564, 0.017571410921370625, 0.6042261134239492, 0.15050046709518608, 0.11401550537514096, 0.19154604903023684, 0.14593984688018044, 0.11401550537514096, 0.13681860645016916, 0.05016682236506202, 0.05016682236506202, 0.05016682236506202, 0.6041500239365434, 0.604166114527951, 0.7278452118688419, 0.11124065856121486, 0.36153214032394826, 0.13905082320151857, 0.08343049392091115, 0.08343049392091115, 0.05562032928060743, 0.05562032928060743, 0.05562032928060743, 0.027810164640303715], \"Term\": [\"aa\", \"aa\", \"aa\", \"abnormal\", \"absoultely\", \"ac\", \"ac\", \"ac\", \"ac\", \"ac\", \"ac\", \"ac\", \"accepted\", \"accord\", \"across\", \"across\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adaptor\", \"adaptor\", \"addicted\", \"againaugust\", \"ajoke\", \"allot\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"amazon\", \"amazon\", \"amazon\", \"amazon\", \"amazon\", \"amazon\", \"amazon\", \"amazon\", \"amazon\", \"amends\", \"angry\", \"anti\", \"antiglare\", \"anybody\", \"arrive\", \"arrive\", \"arrive\", \"arrive\", \"art\", \"asian\", \"atomically\", \"auto\", \"auto\", \"auto\", \"auto\", \"awsome\", \"baby\", \"baby\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"backnevertheless\", \"batterry\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"bb\", \"bedrooms\", \"behaves\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"betteru\", \"bf\", \"bicycle\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetoothgreat\", \"boards\", \"bought\", \"bought\", \"bought\", \"bought\", \"bought\", \"bought\", \"bought\", \"bought\", \"bought\", \"boy\", \"brilliant\", \"brilliant\", \"brilliant\", \"bstrds\", \"bt\", \"bt\", \"bt\", \"bt\", \"bt\", \"bt\", \"bt\", \"bt\", \"bt\", \"bubbles\", \"builtand\", \"burning\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"callings\", \"calls\", \"calls\", \"calls\", \"calls\", \"calls\", \"calls\", \"calls\", \"calls\", \"calls\", \"camper\", \"cancel\", \"cancel\", \"cancel\", \"cancel\", \"cancel\", \"cancel\", \"cant\", \"cant\", \"cant\", \"cant\", \"cant\", \"cant\", \"cant\", \"cant\", \"cant\", \"capability\", \"capability\", \"capability\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"casewith\", \"casing\", \"cassette\", \"cassette\", \"cassette\", \"cassette\", \"cassette\", \"cassette\", \"cassette\", \"cassette\", \"cel\", \"celular\", \"chances\", \"changer\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"charged\", \"charged\", \"charged\", \"charged\", \"charged\", \"charged\", \"charged\", \"charged\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"chargers\", \"chargers\", \"chargers\", \"chargers\", \"chargers\", \"chargers\", \"chargers\", \"chargers\", \"charges\", \"charges\", \"charges\", \"charges\", \"charges\", \"charges\", \"charges\", \"charges\", \"charges\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"charging\", \"cheddar\", \"chevy\", \"chile\", \"choice\", \"choice\", \"choice\", \"clear\", \"clear\", \"clear\", \"clear\", \"clear\", \"clear\", \"clear\", \"clear\", \"clear\", \"closure\", \"closure\", \"clutch\", \"clutch\", \"coffee\", \"cofortable\", \"coil\", \"coints\", \"collect\", \"colorphone\", \"comfortablethese\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"compliments\", \"compliments\", \"confidence\", \"considerations\", \"cord\", \"cord\", \"cord\", \"cord\", \"cord\", \"cord\", \"cord\", \"cord\", \"cord\", \"corresponding\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"counterfiet\", \"covers\", \"covers\", \"covers\", \"covers\", \"covers\", \"covers\", \"crackling\", \"crackling\", \"crackling\", \"crackling\", \"crackling\", \"csm\", \"cyberpowerit\", \"dare\", \"dare\", \"dbm\", \"defect\", \"defect\", \"defensei\", \"definetly\", \"describe\", \"describedfor\", \"descrition\", \"designthe\", \"determine\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"dime\", \"dios\", \"dirt\", \"distraction\", \"dock\", \"dock\", \"dose\", \"dose\", \"doubles\", \"doubles\", \"doubles\", \"doubles\", \"drains\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drives\", \"drives\", \"drives\", \"drives\", \"drives\", \"drives\", \"drives\", \"drives\", \"drivingon\", \"dust\", \"dust\", \"dust\", \"dust\", \"dust\", \"dusty\", \"ear\", \"ear\", \"ear\", \"ear\", \"ear\", \"ear\", \"ear\", \"ear\", \"ear\", \"earpiece\", \"earpiece\", \"earpiece\", \"earpiece\", \"earpiece\", \"earpiece\", \"earpiece\", \"earpiece\", \"earpiece\", \"earset\", \"easy\", \"easy\", \"easy\", \"easy\", \"easy\", \"easy\", \"easy\", \"easy\", \"easy\", \"ebook\", \"eforcity\", \"electronics\", \"electronics\", \"electronics\", \"electronics\", \"electronics\", \"electronics\", \"electronics\", \"electronics\", \"emits\", \"entertainment\", \"este\", \"ether\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"everything\", \"everything\", \"everything\", \"everything\", \"everything\", \"everything\", \"everything\", \"everything\", \"everything\", \"evo\", \"exactly\", \"exactly\", \"exactly\", \"exactly\", \"exactly\", \"exactly\", \"exactly\", \"exactly\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"exceptional\", \"excited\", \"expedition\", \"exspensive\", \"faced\", \"fading\", \"fancy\", \"fancy\", \"fannnnntastic\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"far\", \"fari\", \"fatherinlaw\", \"favorite\", \"favorite\", \"favorite\", \"favorite\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"firefighter\", \"fireworks\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fit\", \"fit\", \"fit\", \"fit\", \"fit\", \"fit\", \"fit\", \"fit\", \"fit\", \"follow\", \"ford\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"found\", \"freeze\", \"frequently\", \"frequently\", \"fro\", \"frsgmrscb\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"functionally\", \"fuse\", \"fuses\", \"fussy\", \"gaps\", \"gasket\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"glare\", \"glove\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"got\", \"gradually\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"greatmake\", \"grief\", \"groves\", \"habit\", \"habra\", \"hace\", \"handset\", \"handset\", \"handset\", \"handset\", \"handset\", \"handset\", \"handset\", \"handset\", \"handsi\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hbs\", \"hbs\", \"hbs\", \"hbs\", \"hbs\", \"hbs\", \"hbs\", \"hbs\", \"hdmi\", \"headset\", \"headset\", \"headset\", \"headset\", \"headset\", \"headset\", \"headset\", \"headset\", \"headset\", \"headsets\", \"headsets\", \"headsets\", \"headsets\", \"headsets\", \"headsets\", \"headsets\", \"headsets\", \"headsets\", \"hexagonal\", \"hides\", \"hole\", \"hole\", \"hole\", \"honda\", \"horriblei\", \"hr\", \"htc\", \"htc\", \"htc\", \"htc\", \"htc\", \"htc\", \"htc\", \"htc\", \"hub\", \"hub\", \"hub\", \"hub\", \"hub\", \"hub\", \"hub\", \"hub\", \"hub\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"im\", \"im\", \"im\", \"im\", \"im\", \"im\", \"im\", \"im\", \"im\", \"impressedi\", \"impressedi\", \"impressedi\", \"incase\", \"info\", \"initiate\", \"inserts\", \"intended\", \"intended\", \"intended\", \"intended\", \"intended\", \"intended\", \"intended\", \"intended\", \"interrupting\", \"inverter\", \"inverter\", \"inverter\", \"inverter\", \"inverter\", \"iohone\", \"ip\", \"ipad\", \"ipad\", \"ipad\", \"ipad\", \"iphoe\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphonesi\", \"iphonethanks\", \"issuei\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"itgreat\", \"itnoteit\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"ive\", \"jawbone\", \"jawbone\", \"jawbone\", \"jawbone\", \"jawbone\", \"jawbone\", \"jealous\", \"jobwould\", \"judy\", \"jut\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keyboard\", \"keyboard\", \"keyboard\", \"keyboard\", \"keyboard\", \"keyboard\", \"keyboard\", \"keyboard\", \"keyboard\", \"keys\", \"keys\", \"keys\", \"keys\", \"keys\", \"keys\", \"keys\", \"keys\", \"keys\", \"kids\", \"lacks\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"latch\", \"late\", \"learnedcheap\", \"lest\", \"lest\", \"levana\", \"lg\", \"lg\", \"lg\", \"lg\", \"lg\", \"lg\", \"lg\", \"lg\", \"lg\", \"lifters\", \"liion\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"listeners\", \"listeners\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"logitech\", \"logitech\", \"logitech\", \"logitech\", \"logitech\", \"logitech\", \"logitech\", \"logitech\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"lookingthis\", \"loops\", \"loss\", \"loss\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"loved\", \"loved\", \"loved\", \"loved\", \"loved\", \"loved\", \"machining\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"magnetic\", \"magnetic\", \"magnetic\", \"magnetic\", \"magnetic\", \"mail\", \"mail\", \"mail\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"markings\", \"markings\", \"mate\", \"materials\", \"materials\", \"materials\", \"matters\", \"maxell\", \"maxell\", \"mc\", \"mc\", \"mc\", \"meant\", \"meant\", \"meant\", \"meant\", \"meant\", \"meant\", \"medical\", \"mic\", \"mic\", \"mic\", \"mic\", \"mic\", \"mic\", \"mic\", \"mic\", \"mic\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"mikey\", \"million\", \"min\", \"min\", \"minus\", \"mix\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"moneygobbling\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"motorola\", \"motorola\", \"motorola\", \"motorola\", \"motorola\", \"motorola\", \"motorola\", \"motorola\", \"motorola\", \"motorolagiant\", \"mouth\", \"mouth\", \"mouth\", \"mouth\", \"mouth\", \"mouth\", \"mouth\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"multitasking\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"must\", \"neckband\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"needed\", \"needed\", \"needed\", \"needed\", \"needed\", \"needed\", \"needed\", \"needed\", \"needed\", \"needing\", \"needing\", \"needing\", \"nephew\", \"nervous\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"newest\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"nose\", \"nutshell\", \"oem\", \"oem\", \"oem\", \"oem\", \"oem\", \"oem\", \"oem\", \"oem\", \"oem\", \"offered\", \"office\", \"office\", \"office\", \"office\", \"office\", \"office\", \"office\", \"office\", \"office\", \"official\", \"offline\", \"oldfashioned\", \"omg\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"open\", \"otherthings\", \"paint\", \"paint\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"passengers\", \"patient\", \"pda\", \"pda\", \"pda\", \"pda\", \"pda\", \"pda\", \"pda\", \"pda\", \"pda\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"percent\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfection\", \"perfectly\", \"perfectly\", \"perfectly\", \"perfectly\", \"perfectly\", \"perfectly\", \"perfectly\", \"perfectly\", \"perfectly\", \"periodically\", \"periodically\", \"periodically\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phonefits\", \"phones\", \"phones\", \"phones\", \"phones\", \"phones\", \"phones\", \"phones\", \"phones\", \"phones\", \"pice\", \"pieces\", \"pieces\", \"pieces\", \"pieces\", \"pieces\", \"pieces\", \"pieces\", \"pieces\", \"pieces\", \"pinhole\", \"plantronics\", \"plantronics\", \"plantronics\", \"plantronics\", \"plantronics\", \"plantronics\", \"plantronics\", \"plantronics\", \"plantronics\", \"plastic\", \"plastic\", \"plastic\", \"plastic\", \"plastic\", \"plastic\", \"plastic\", \"plastic\", \"plastic\", \"player\", \"player\", \"player\", \"player\", \"player\", \"player\", \"player\", \"player\", \"player\", \"plug\", \"plug\", \"plug\", \"plug\", \"plug\", \"plug\", \"plug\", \"plug\", \"plug\", \"plugged\", \"plugged\", \"plugged\", \"plugged\", \"plugged\", \"plugged\", \"plugged\", \"plugged\", \"plugged\", \"pnone\", \"pointrather\", \"portal\", \"ports\", \"ports\", \"ports\", \"ports\", \"ports\", \"ports\", \"ports\", \"ports\", \"ports\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"powerd\", \"preexisting\", \"prettyand\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"pricesave\", \"priceyou\", \"prints\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problemsi\", \"proclaimed\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"productsfree\", \"prone\", \"protecting\", \"protecting\", \"protector\", \"protector\", \"protector\", \"protector\", \"protector\", \"protector\", \"protector\", \"pry\", \"pry\", \"pry\", \"pry\", \"pry\", \"pry\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchased\", \"purchased\", \"purchased\", \"purchased\", \"purchased\", \"purchased\", \"purchased\", \"purchased\", \"purchased\", \"quaility\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quickcharge\", \"quiet\", \"quiet\", \"quiet\", \"quiet\", \"quiet\", \"quiet\", \"quits\", \"radios\", \"ranging\", \"raving\", \"razr\", \"razr\", \"razr\", \"razr\", \"razr\", \"razr\", \"razr\", \"razr\", \"razr\", \"reaching\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"recepion\", \"reception\", \"reception\", \"reception\", \"reception\", \"reception\", \"reception\", \"reception\", \"reception\", \"reception\", \"receptionon\", \"receptionupdate\", \"recomend\", \"recomendablenote\", \"recycled\", \"reel\", \"repair\", \"repair\", \"repair\", \"replace\", \"replace\", \"replace\", \"replace\", \"replace\", \"replace\", \"replace\", \"replace\", \"replace\", \"replacedto\", \"resolved\", \"retracted\", \"retracted\", \"reverse\", \"reverse\", \"reverse\", \"rpms\", \"rubber\", \"rubber\", \"rubber\", \"rubber\", \"rubber\", \"rubber\", \"rubber\", \"rubber\", \"rubber\", \"sabrent\", \"sabrent\", \"sabrent\", \"sabrent\", \"sabrent\", \"safe\", \"scheduled\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screwdrivers\", \"screwdrivers\", \"screwdrivers\", \"screwdrivers\", \"screwdrivers\", \"screwdrivers\", \"screws\", \"screws\", \"screws\", \"screws\", \"screws\", \"screws\", \"screws\", \"scuff\", \"searched\", \"searching\", \"searching\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"self\", \"seller\", \"seller\", \"seller\", \"seller\", \"seller\", \"seller\", \"seller\", \"seller\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"setfills\", \"sgsii\", \"shapetightness\", \"shelf\", \"shelf\", \"shelf\", \"shining\", \"shipment\", \"side\", \"side\", \"side\", \"side\", \"side\", \"side\", \"side\", \"side\", \"side\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"sii\", \"sii\", \"siquiera\", \"sirus\", \"sisters\", \"six\", \"sk\", \"sk\", \"sk\", \"skill\", \"slider\", \"smoking\", \"snapped\", \"soaked\", \"softer\", \"solvedi\", \"soomething\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"speaking\", \"speaking\", \"speaking\", \"speaking\", \"specially\", \"specified\", \"spen\", \"spen\", \"spigen\", \"spongy\", \"spudger\", \"ssuper\", \"starpluss\", \"staying\", \"stickers\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"stopped\", \"stopped\", \"stopped\", \"stopped\", \"stopped\", \"stopped\", \"stopped\", \"strength\", \"strength\", \"strength\", \"strength\", \"strength\", \"strength\", \"strength\", \"strikesas\", \"sts\", \"suburbani\", \"sudden\", \"suited\", \"supper\", \"supposed\", \"supposed\", \"supposed\", \"supposed\", \"supposed\", \"supposed\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"symbols\", \"symbols\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"tab\", \"tab\", \"tablet\", \"tablet\", \"tablet\", \"talkabout\", \"tapeproblem\", \"teamspeak\", \"terribly\", \"thank\", \"thank\", \"thank\", \"thank\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thelg\", \"theseit\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"til\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timewall\", \"tip\", \"tip\", \"tip\", \"tis\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"tone\", \"tone\", \"tone\", \"tone\", \"tone\", \"tone\", \"tone\", \"tone\", \"tones\", \"tones\", \"tones\", \"tones\", \"tones\", \"tones\", \"tones\", \"tones\", \"tons\", \"took\", \"took\", \"took\", \"took\", \"took\", \"took\", \"took\", \"took\", \"took\", \"tooth\", \"tooth\", \"tooth\", \"tooth\", \"tooth\", \"tothere\", \"touchpads\", \"transactions\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmit\", \"transmitter\", \"transmitter\", \"transmitter\", \"transmitting\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"tried\", \"tried\", \"tried\", \"tried\", \"tried\", \"tried\", \"tried\", \"tried\", \"tried\", \"truck\", \"tunnel\", \"tunnel\", \"tunnel\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"underpowered\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unitplug\", \"unpack\", \"unreliable\", \"upgrated\", \"upset\", \"upset\", \"usa\", \"usa\", \"usb\", \"usb\", \"usb\", \"usb\", \"usb\", \"usb\", \"usb\", \"usb\", \"usb\", \"usd\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"usenot\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"vehicles\", \"volume\", \"volume\", \"volume\", \"volume\", \"volume\", \"volume\", \"volume\", \"volume\", \"volume\", \"voyager\", \"waaaaaaay\", \"walkietalkie\", \"walkietalkie\", \"walkman\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"waste\", \"waste\", \"waste\", \"waste\", \"waste\", \"waste\", \"waterproof\", \"waterproof\", \"watts\", \"watts\", \"watts\", \"wax\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"wedge\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"welli\", \"wep\", \"whoever\", \"whooshing\", \"whos\", \"wiggle\", \"wiggle\", \"wilson\", \"wilson\", \"wilson\", \"wilson\", \"wilson\", \"win\", \"win\", \"wish\", \"wish\", \"wish\", \"wish\", \"wish\", \"wish\", \"wish\", \"wish\", \"wish\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"worked\", \"worked\", \"worked\", \"worked\", \"worked\", \"worked\", \"worked\", \"worked\", \"worked\", \"working\", \"working\", \"working\", \"working\", \"working\", \"working\", \"working\", \"working\", \"working\", \"workingthats\", \"works\", \"works\", \"works\", \"works\", \"works\", \"works\", \"works\", \"works\", \"works\", \"worrying\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"wrapping\", \"yagi\", \"yellowed\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [8, 6, 9, 5, 1, 3, 4, 2, 7]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el796661403455920927045201112529\", ldavis_el796661403455920927045201112529_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el796661403455920927045201112529\", ldavis_el796661403455920927045201112529_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el796661403455920927045201112529\", ldavis_el796661403455920927045201112529_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ukn2e3iWRoIz"
   },
   "source": [
    "# 2.word2Vec [40pts]\n",
    "\n",
    "\n",
    "In this problem, we use Amazon Review Dataset to perform Word2Vec and Doc2Vec to extract insights relevant for e-commerce business. For this question, download and use the dataset [here](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz||reviews_Electronics_5.json.gz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UKcbiFCRuJr"
   },
   "source": [
    "## 2.1 Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcw3Abp6R549"
   },
   "source": [
    "The following code reads the data from a GZIP file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T-HJ5AnnRpFB"
   },
   "outputs": [],
   "source": [
    "# A function to read the zipped data at a specfic path\n",
    "#\n",
    "# How to use:\n",
    "# PATH = \"/path/to/file\"\n",
    "# for line in parse(PATH):\n",
    "#   do something with line\n",
    "#\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O543cF24R81Z"
   },
   "source": [
    "We will now read the data and preprocess it using the following steps:\n",
    "\n",
    "   1. Remove stopwords\n",
    "   2. Lower-case all words\n",
    "   3. Remove words with less than 2 characters\n",
    "   4. Remove punctuation\n",
    "   5. Split each sentence into a list of words\n",
    "\n",
    "   And finally extract 10000 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "id": "4_GNZ3ikR_rE"
   },
   "outputs": [],
   "source": [
    "# A function to clean a single line of text\n",
    "def clean_line(line):\n",
    "    \"\"\" Clean stopwords and punction for each line\n",
    "    \n",
    "    Args: \n",
    "        line (string): one line in file\n",
    "        \n",
    "    Returns:\n",
    "        list(str): a list of all words in the sentence\n",
    "    \"\"\"\n",
    "    line = line.split(\" \")\n",
    "    filtered_content = []\n",
    "    for word in line:\n",
    "        #########################\n",
    "        if word.lower() not in stopWords:\n",
    "            word = word.lower()\n",
    "            word = re.sub(punctuationRegex, '', word)\n",
    "        if word not in stopWords and len(word) >= 2:\n",
    "            filtered_content.append(word)\n",
    "        #########################\n",
    "    return filtered_content\n",
    "\n",
    "def read_dataset(fname):\n",
    "    \"\"\" Read the 100000 lines in given dataset into list and clean stop words. \n",
    "        \n",
    "    Args: \n",
    "        fname (string): filename of Amazon Review Dataset\n",
    "        \n",
    "    Returns:\n",
    "        list of list of words: we view each document as a list, including a list of all words \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    exp_dataset = []\n",
    "    for review in parse(fname):\n",
    "        line = review[\"reviewText\"]\n",
    "        #to see the raw lines\n",
    "        print(line)\n",
    "        new_line = clean_line(line)\n",
    "        exp_dataset.append(new_line)\n",
    "        count += 1\n",
    "        if count > 100000:\n",
    "            break\n",
    "    return exp_dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zaf3YFgHTNr3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.9 s, sys: 602 ms, total: 19.5 s\n",
      "Wall time: 19.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['got',\n",
       "  'gps',\n",
       "  'husband',\n",
       "  'otr',\n",
       "  'road',\n",
       "  'trucker',\n",
       "  'impressed',\n",
       "  'shipping',\n",
       "  'time',\n",
       "  'arrived',\n",
       "  'days',\n",
       "  'earlier',\n",
       "  'expected',\n",
       "  'within',\n",
       "  'week',\n",
       "  'use',\n",
       "  'however',\n",
       "  'started',\n",
       "  'freezing',\n",
       "  'could',\n",
       "  'glitch',\n",
       "  'unit',\n",
       "  'worked',\n",
       "  'great',\n",
       "  'worked',\n",
       "  'work',\n",
       "  'great',\n",
       "  'normal',\n",
       "  'person',\n",
       "  'well',\n",
       "  'trucker',\n",
       "  'option',\n",
       "  'big',\n",
       "  'truck',\n",
       "  'routes',\n",
       "  'tells',\n",
       "  'scale',\n",
       "  'coming',\n",
       "  'ect',\n",
       "  'love',\n",
       "  'bigger',\n",
       "  'screen',\n",
       "  'ease',\n",
       "  'use',\n",
       "  'ease',\n",
       "  'putting',\n",
       "  'addresses',\n",
       "  'memory',\n",
       "  'nothing',\n",
       "  'really',\n",
       "  'bad',\n",
       "  'say',\n",
       "  'unit',\n",
       "  'exception',\n",
       "  'freezing',\n",
       "  'probably',\n",
       "  'one',\n",
       "  'million',\n",
       "  'thats',\n",
       "  'luck',\n",
       "  'contacted',\n",
       "  'seller',\n",
       "  'within',\n",
       "  'minutes',\n",
       "  'email',\n",
       "  'received',\n",
       "  'email',\n",
       "  'back',\n",
       "  'instructions',\n",
       "  'exchange',\n",
       "  'impressed',\n",
       "  'way',\n",
       "  'around']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "stopWords = set(stopwords.words('english'))\n",
    "punctuationRegex = r'\\W+|\\d+'\n",
    "r = read_dataset(\"reviews_Electronics_5.json.gz\")\n",
    "r[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFMVOQgrTi9s"
   },
   "source": [
    "**[3pts]2.2.1** In this question, first we will build a Word2Vec model using ginsim using size=300, min_count=40, win- dow=10, negative=10, max_vocab_size=10000. Train the model for 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "5gGK2QnKTfdO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 25s, sys: 4.29 s, total: 17min 29s\n",
      "Wall time: 5min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(133138401, 163409010)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "# YOUR CODE HERE\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=r,vector_size=300,min_count=40,\n",
    "                                      window=10,negative=10,max_vocab_size=10000)\n",
    "w2v.train(r, total_examples=w2v.corpus_count,epochs=30)\n",
    "\n",
    "#w2v = gensim.models.KeyedVectors.load_word2vec_format(r, binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPnJE0pQT6jr"
   },
   "source": [
    "**[2pts]2.2.2** Use model.wv.doesnt_match to find a word in [\"Canon\",\"Nikon\",\"junk\"] that does not\n",
    "\n",
    "belong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "pGQT-f_OTqnX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'junk'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE [\"Canon\", \"Nikon\", \"junk\"]\n",
    "word_vectors = w2v.wv\n",
    "word_vectors.doesnt_match([\"Canon\", \"Nikon\", \"junk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a44R5v_-UFXp"
   },
   "source": [
    "**[3pts]2.2.3** Come up with 3 other word lists and apply the above function. Explain your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "id": "GP9dTi1yUNGR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pen'"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "word_vectors.doesnt_match([\"Burger King\", \"McDonalds\",\"pen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlZKiD_DUN00"
   },
   "source": [
    "-I used Berger King, Mcdonalds, and pen. As expected, pen does not belong to be in the same group with the two of the well-known fastfood chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLz_WXiHUG4L"
   },
   "source": [
    "**[2pts]2.2.4** What are some tasks in e-commerce that can be solved with this simple function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjaKCXMEUYyM"
   },
   "source": [
    "-This simple function can be implemented in Amazon search engines for help customers to find what they looking for easily after typing down couple of keywords. Also it can disect the relevant words and excludes the ones that are not really relevant given a specific prompt. More specifically, it can be used to identify irrelevant products in a search query or to identify and remove outliers in a dataset. For product recommendations, it can also be used to identify items that are not related to the user's interests or purchase history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZmhD59iUtR3"
   },
   "source": [
    "## 2.3 Build a doc2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[15 pts] 2.3.1**  Each review is marked by other customers as helpful or not. The \"helpful: [a, b]\" item in each review is (a) the number of people who marked the review as helpful, and (b) the total number of people who have marked the review as helpful or unhelpful. The \"helpfulness\" score of a review can be calculated as a/b. Define a \"helpful\" review as one with helpfulness score >= 0.8. Given a review that is only slightly helpful, could we find textually similar reviews but have higher helpfulness? Build Doc2Vec model with gensim on review data. Use product ID B00006I5WJ and ReviewerID with A14453U0KFWF31 as an example, find top 5 helpful reviews of the same product with similarity score above 0.8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "id": "xGDDL9a4UBGi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def read_reviewers_data(fname, min_count=0):\n",
    "    '''\n",
    "    Save all reviews into their own product asin files.\n",
    "    Make sure you have 'product' folder when you run this answer.\n",
    "    In each file, you can choose your own log structure. In this answer, log strucutre is like \n",
    "        \"reviewText\"\\t\"reviewerID\"\\t\"helpful\"\n",
    "    Args: \n",
    "        fname: dataset file path\n",
    "        min_count: minimum number of reviews of a product\n",
    "    Returns:\n",
    "        none\n",
    "    '''\n",
    "    if not os.path.isdir('product'):\n",
    "        os.makedirs('product')\n",
    "    asin_list = []\n",
    "    tmp_list = []\n",
    "    last_asin = \"\"\n",
    "    j = 0\n",
    "    for i in parse(fname):\n",
    "        if last_asin != i['asin']:\n",
    "            if len(tmp_list) > min_count:\n",
    "                f = open(\"product/\" + last_asin+\".txt\", 'w')\n",
    "                for one in tmp_list:\n",
    "                    f.write(one)\n",
    "                f.close()\n",
    "            tmp_list = []\n",
    "            last_asin = i['asin']\n",
    "        tmp_list.append(i[\"reviewText\"] + '\\t' + i[\"reviewerID\"] +\n",
    "                    '\\t' + handle_helpful(i[\"helpful\"]) + \"\\n\")\n",
    "        j += 1\n",
    "        if j > 100000:\n",
    "            break\n",
    "    \n",
    "def handle_helpful(helpful):\n",
    "    '''\n",
    "    Helper function for helpful_score calculate\n",
    "    Args: \n",
    "        helpful: list. The first element is the number of people think this is helpful. The second element\n",
    "            is the total number of people evaluate this comment\n",
    "    Returns:\n",
    "        String: number represent helpfulness\n",
    "    '''\n",
    "    if helpful[1] != 0:\n",
    "        helpfulness = 1.0 * helpful[0] / helpful[1]\n",
    "        return str(helpfulness)\n",
    "    else:\n",
    "        return str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "id": "ZcaWNGSyban3"
   },
   "outputs": [],
   "source": [
    "read_reviewers_data(\"reviews_Electronics_5.json.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "id": "phyr9d9Zbvbe"
   },
   "outputs": [],
   "source": [
    "class TaggedReviewDocument(object):\n",
    "    '''\n",
    "    This class could save all products and review information in its dictionary and generate iter for TaggedDocument\n",
    "        which could used for Doc2Vec model\n",
    "    '''\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "        self.helpfulness = {}  # key:reviewerID value:helpfulness\n",
    "        self.product = {}      # key:asin value:reviewerID\n",
    "        self.asin = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        for filename in os.listdir(self.dirname):\n",
    "            asin_code = filename[:-4] #delete \".txt\"\n",
    "            self.product[asin_code] = []\n",
    "            self.asin.append(asin_code)\n",
    "            for line in enumerate(open(self.dirname + \"/\" + filename)):\n",
    "                line_content = line[1].split(\"\\t\")\n",
    "                self.product[asin_code].append(line_content[1])\n",
    "                self.helpfulness[line_content[1]] = float(line_content[2])\n",
    "                yield TaggedDocument(clean_line(line_content[0]), [line_content[1], line_content[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "id": "59Ccalf7bwE2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TaggedReviewDocument at 0x7fb35270b910>"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = TaggedReviewDocument(\"product\")\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "id": "crSL0O9HbyPI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 44s, sys: 1min 25s, total: 22min 10s\n",
      "Wall time: 14min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "# YOUR CODE HERE\n",
    "\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "model.build_vocab(documents)\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=30)\n",
    "#target_vector = model.infer_vector(nltk.word_tokenize(target_review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x7fb383fab7c0>"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbabO2WUcZxJ"
   },
   "source": [
    "## Find similar reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def find_similar_reviews(asin,reviewer_id):\n",
    "    '''\n",
    "    If one review is similar to the specefic review and it is much helpful, save it to a list\n",
    "    Args: \n",
    "        asin: product asin\n",
    "        reviewer_id: the specific review\n",
    "    Returns:\n",
    "        list of reviewer id\n",
    "    '''\n",
    "    result = []\n",
    "    ########################\n",
    "    lst = []\n",
    "    sim = 0\n",
    "    \n",
    "\n",
    "    with open(\"product/\" +asin+\".txt\", mode='r') as product_file:\n",
    "        reader = csv.reader(product_file, delimiter='\\t')\n",
    "        for r in reader:\n",
    "            #using for loop to get each review that belongs to \n",
    "            #specific asin code which is ('B00006I5WJ')\n",
    "            (reviewText, reviewerID, helpscore) = r\n",
    "            #applying conditions like having helpscore greater than 0.8\n",
    "            #and for not getting the same reviewer ID ('A14453U0KFWF31')\n",
    "            if float(helpscore) > 0.8 and reviewerID != reviewer_id:\n",
    "                sim = model.dv.similarity(reviewerID, reviewer_id)\n",
    "                #if similarity score of a specific review belongs to\n",
    "                #to same product is higher than 0.8, append it to empty list\n",
    "                if sim > 0.8:\n",
    "                    lst.append((reviewerID, sim))\n",
    "    \n",
    "    cnt = 0\n",
    "    #sorting the list of key value pairs\n",
    "    #in decreasing order to get the top 5 in first five iterations:\n",
    "    lst = sorted(lst, key = lambda s: s[1], reverse = True)\n",
    "    for idx, sim in lst:\n",
    "        if cnt < 5:\n",
    "            result.append((idx, sim))\n",
    "            cnt = cnt+1\n",
    "    ########################\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AOUKRECWP828A', 0.85026777),\n",
       " ('A6FIAB28IS79', 0.83004725),\n",
       " ('A5C7KD02LS69I', 0.8196159),\n",
       " ('AGITH5SMFTBOB', 0.80659777),\n",
       " ('A3I4TQNEO4G6LT', 0.80619234)]"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_reviews('B00006I5WJ','A14453U0KFWF31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KIVR0OodJbc"
   },
   "source": [
    "## 2.4 Build a doc2vec model using product descriptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aP5_XP5dO2i"
   },
   "source": [
    "**[10pts]2.4.1** Use product descriptions (located in meta data  [here](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz)) to build a Doc2Vec model. When building the doc2vec model, use vector_size=100, window=15, min_count=5, max_vocab_size=1000, and train it for 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6j9r0LvidJ7V"
   },
   "outputs": [],
   "source": [
    "def read_product_description(fname):\n",
    "    '''\n",
    "    Load all product descriptions\n",
    "    Args: \n",
    "        fname: dataset file path\n",
    "    Returns:\n",
    "        dict: key is asin, value is description content\n",
    "    '''\n",
    "    result = {}\n",
    "    for i in parse(fname):\n",
    "        try:\n",
    "            if \"Camera & Photo\" in i[\"categories\"][0]:\n",
    "                result[i[\"asin\"]]=i[\"description\"]\n",
    "        except:\n",
    "            continue\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "XrSISgv3df6_"
   },
   "outputs": [],
   "source": [
    "class TaggedDescriptionDocument(object):\n",
    "    '''\n",
    "    This class could save all products and review information in its dictionary and generate iter for TaggedDocument\n",
    "        which could used for Doc2Vec model\n",
    "    '''\n",
    "    def __init__(self, descriptondict):\n",
    "        self.descriptondict = descriptondict\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        for asin in self.descriptondict:\n",
    "            for content in self.descriptondict[asin]:\n",
    "                yield TaggedDocument(clean_line(content), [asin])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "id": "yTRwOCtldinm"
   },
   "outputs": [],
   "source": [
    "description_dict = read_product_description(\"meta_Electronics.json.gz\")\n",
    "des_documents = TaggedDescriptionDocument(description_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0eG6BC6oeNUA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# A function to replace 'new lines' in the text with space and to remove 'returns'.\n",
    "def clean_text(text):\n",
    "    cleaned_text = text.lower().replace('\\n',' ').replace('\\r','')\n",
    "    return cleaned_text\n",
    "\n",
    "# A function to generate tagged documents, a list that contains key-value pairs\n",
    "# of cleaned version of product description and their corresponding product asin codes.\n",
    "def create_tagged_documents(description_dict):\n",
    "    tagged_documents = []\n",
    "    for asin in description_dict:\n",
    "        tagged_documents.append(TaggedDocument(words=clean_text(description_dict[asin]).split(), tags=[asin]))\n",
    "    return tagged_documents\n",
    "\n",
    "    \n",
    "description_dict = read_product_description(\"meta_Electronics.json.gz\")\n",
    "\n",
    "tagged_documents = create_tagged_documents(description_dict)\n",
    "\n",
    "model = Doc2Vec(vector_size = 100, window=15, min_count = 5, max_vocab_size = 1000, epochs =1)\n",
    "model.build_vocab(tagged_documents)\n",
    "model.train(tagged_documents,total_examples=model.corpus_count, epochs = model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvwRTr6ueRS4"
   },
   "source": [
    "**[5pts]2.4.2** Find the most similar product for Canon EOS 5D (asin:B0007Y791C) not made by Canon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "wrQZo-AleN-O"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qr/b2njs26s0wgc0pzwtq_b3t440000gn/T/ipykernel_79666/1821483855.py:15: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similar_products = model.docvecs.most_similar([vector],topn=len(model.docvecs))\n",
      "/var/folders/qr/b2njs26s0wgc0pzwtq_b3t440000gn/T/ipykernel_79666/1821483855.py:15: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similar_products = model.docvecs.most_similar([vector],topn=len(model.docvecs))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "product_name = \"\"\n",
    "product_description = \"\"\n",
    "\n",
    "for asin in description_dict:\n",
    "    if asin == \"B0007Y791C\":\n",
    "        product_name = description_dict[asin]\n",
    "        product_description = clean_text(product_name)\n",
    "        break\n",
    "# Vector representation for CANON EOS 5D\n",
    "vector = model.infer_vector(product_description.split())\n",
    "# Cosine similarities\n",
    "similar_products = model.docvecs.most_similar([vector],topn=len(model.docvecs))\n",
    "\n",
    "most_similar_product = None\n",
    "for product in similar_products:\n",
    "    asin = product[0]\n",
    "    for p in description_dict:\n",
    "        #filtering out the products made by Canon to get the most \n",
    "        #similar product that is not made by Canon\n",
    "        if p == asin and 'Canon' not in description_dict[p]:\n",
    "            most_similar_product = asin\n",
    "            break\n",
    "    if most_similar_product is not None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Ild0RccPeVU3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar product to Canon EOS 5D, not made by Canon is B007X47WGG\n"
     ]
    }
   ],
   "source": [
    "print('Most similar product to Canon EOS 5D, not made by Canon is',most_similar_product)\n",
    "# Exact product name is Nikon D3200 Digital SLR Camera &amp"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
